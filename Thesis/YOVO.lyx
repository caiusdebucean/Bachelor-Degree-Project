#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{hyperref}
\usepackage[linesnumbered,ruled,vlined,algochapter]{algorithm2e}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}
%\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}{}}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\textit{\nouppercase{\leftmark}}}
\fancyhead[RE]{\textit{\nouppercase{\leftmark}}}

\renewcommand{\rmdefault}{ptm}

\renewenvironment{proof}{{\flushleft\itshape Rezolvare.}}{\qed}


\usepackage{babel}
  \providecommand{\proofname}{Rezolvare}
  \providecommand{\algorithmname}{Algoritmul}
  \providecommand{\definitionname}{Defini\c{t}ia}
  \providecommand{\examplename}{Exemplul}
  \providecommand{\factname}{Faptul}
  \providecommand{\lemmaname}{Lema}
  \providecommand{\corollaryname}{Corolarul}
  \providecommand{\theoremname}{Teorema}
  \providecommand{\problemname}{Exerci\c{t}iul}
\end_preamble
\use_default_options false
\begin_modules
eqs-within-sections
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-chap-bytype
\end_modules
\maintain_unincluded_children false
\language romanian
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style polish
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle empty
\bullet 1 1 34 -1
\bullet 2 2 35 -1
\bullet 3 2 7 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
YOVO: You Only Voxelize Once
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mbox{} 
\backslash
thispagestyle{empty} 
\backslash
newpage
\backslash
setcounter{page}{3}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introducere
\end_layout

\begin_layout Section
Contextul problemei
\end_layout

\begin_layout Standard
Reconstituirea digitala al unui obiect reprezinta o problema propusa de
 cateva decenii si este activ tratata in domeniul Viziunii Artificiale si
 al Graficii Computerizate, avand ca scop final obtinerea unei forme cat
 mai fidele al obiectulor reale.
 Procedeele existente se folosesc de varii tipuri de date pentru obtinerea
 unei reconstructii, acestea fiind obtinute prin uzul de tehnologii precum:
 Camere clasice, Camere RGB-D, LiDaR, Raze X, Ultrasunete, RMN, CT etc.
 Considerand costul aferent fiecarei alternative prin hardware-ul dedicat
 necesar, se poate deduce ca cea mai ieftina solutie ar necesita o simpla
 camera RGB, cu viziune monotipica, pentru a creea imagini 2D.
 Cu toate ca celelalte tipuri de date pot conduce la reproduceri volumetrice
 mai robuste, economisirea resurselor este un aspect ce concretizeaza interesul
 existent in solutii ce folosesc doar imagini.
\end_layout

\begin_layout Standard
Recuperarea dimensiunii a 3-a din poze 2D a fost telul multor studii in
 ultimii ani.
 Prima generatie de metode au tratat perspectiva geometrica din punct de
 vedere matematic, observand proiectia 2D al obiecteor 3D, si incercand
 sa creeze un proces reversibil.
 Solutiile cele mai bune ale acestei abordari necesita multiple cadre ce
 capturau diferite unghiuri ale obiectelor, acestea avand nevoie de o calibrare
 meticuloasa.
 A doua generatie de metode a folosit o memorie ce continea cunostinte anterioar
e despre obiecte.
 Se poate trage o paralela la capabilitatea umana prin care este posibila
 deducerea formei si geometriei unui obiect folosind si doar un singur ochi,
 cu ajutorul cunostintelor precedente despre obiecte asemanatoare celui
 in cauza.
 Din acest punct de vedere, problema de reconstructie al obiectelor 3D devine
 o problema de recunoastere.
 Considerand in prezent eficienta solutilor de Invatare Profunda pentru
 problemele de recunoastere, cat si cresterea cuantumului de noi date ce
 pot fi folosite drept date de antrenare, se justifica tendinta comunitatii
 stiintifice de a folosi ramuri ale Invatarii Profunde precum Retele Neuronale
 Convolutionale sau Retele Neuronale Recurente pentru obtinerea geometriei
 si structurii 3D al obiectelor.
 
\end_layout

\begin_layout Section
Soluția propusa
\end_layout

\begin_layout Standard
Aceasta lucrare abordeaza reconstructia 3D a obietelor print intermediul
 retelelor neuronale, folosind doar una sau mai multe imagini 2D.
 Deoarece natura acestei probleme face parte din domeniul Viziunii Artificiale,
 sunt folosite preponderent Retele Neuronale Convolutionale.
 De asemenea, exista 3 mari tipuri de volume ce pot fi reconstruite: ansamblu
 de fasii, ansamblu de puncte si ansamblu de voxeli.
 Solutia actuala se foloseste de ultimul tip de volum prezentat.
\end_layout

\begin_layout Standard
Arhitectura propusa este compusa din 3 module: 
\end_layout

\begin_layout Itemize
Auto-codificator: format dintr-un codificator ce extrage diferite trasaturi
 ale imaginii primite si un decodificator ce interpreteaza trasaturile extrase
 in volume voxelizate
\end_layout

\begin_layout Itemize
Unificator: realizeaza contopirea multiplelor volume deduse intr-un volum
 mai robust
\end_layout

\begin_layout Itemize
Rafinor: realizeaza cizelarea volumului unificat
\end_layout

\begin_layout Standard
Spre deosebire de solutii din aceeasi familie precum Pix2Vox , ce aloca
 un Auto-Codificator complet convolutional clasic, 3D-R2N2 , ce introduce
 aspecte recurente pentru tratarea cazurilor cu multiple poze, prin blocuri
 LSTM 3D Convolutionale si GRU 3D Convolutionale, solutia prezentata introduce,
 din punct de vedere arhitectural, nivele aditionale de convolutie al hartilor
 de caracteristici la ultimele 3 trepte ale Codificatorului, rezultand in
 3 volume voxelizate ce captureaza diferite reconstructii are obiectului
 real.
 Dupa trecerea prin celelalte module ale arhitecturii, rezultatul final
 este un singur volum.
 Aditional, adaugarea si integrarea procedeelor precum extractor de caracteristi
ci MobileNet V2, functii de activare Mish, regularizare DropBlock si optimizator
 Ranger aduc rezultatele lui YOVO peste SotA-ul actual pe datasetul Data3D−R2N2.
\end_layout

\begin_layout Standard
Cu ajutorul bibliotecii kaolin, volumele pot fi vizualizate intr-un mediu
 3D interactiv, in care se poate analiza din orice unghi volumul voxelizat
 reconstruit.
 De asemenea, este posibila reprezentarea acestuia sub forma unui ansamblu
 de plase, pentru o reprezentare mai neteda.
\end_layout

\begin_layout Chapter
Fundamente teoretice
\end_layout

\begin_layout Section
Inteligenta Artificiala si subdomeniile ei
\end_layout

\begin_layout Standard
Odata cu cresterea popularitatii solutiilor de Inteligenta Artificiala s-a
 creat un nivel ridicat de confuzie despre cum se defineste si diferentiaza
 Inteligenta Artificiala de alte concepte precum Invatare Automata, Invatare
 Profunda si Viziune Artificiala.
 Se poate observa in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "IA_domenii"
plural "false"
caps "false"
noprefix "false"

\end_inset

 structura subdomeniilor Inteligentei Artificiale.
\end_layout

\begin_layout Standard
Inteligenta artificiala poate fi interpretata drept incorporarea inteligentei
 umane in masini.
 Aceasta include si cele mai simple exemple de solutii implementate pe un
 calculator, cum ar fi sistemele definite de reguli simple.
\end_layout

\begin_layout Standard
Invatarea Automata reprezinta un subdomeniu al Inteligentei Artificiale,
 reprezentand abilitatea calculatoarelor de a 
\begin_inset Quotes pld
\end_inset

invata
\begin_inset Quotes prd
\end_inset

 sa rezolve o problema fara sa a avea explicit programate fiecare instructiune.
 Cu cat sistemul este mai expus la un cuantum mai mare de date, cu atat
 mai mult algoritmul de invatare automata se auto-regleaza.
\end_layout

\begin_layout Standard
Invatarea Profunda este la randul sau un subdomeniu al Invatarii Automate
 si descrie o tehnica de rezolvare a problemelor cu retele neuronale, structuri
 inspirate de sistemul cerebral umana.
 Spre deosebire de celelalte tehnici alternative, invatarea profunda necesita
 mai multe resurse hardware, in functie de profunzimea si densitatea retelelor
 folosite.
 
\end_layout

\begin_layout Standard
Viziunea Artificiala este un domeniu al Informaticii ce are ca scop dezvoltarea
 abilitatilor calculatoarelor de a identifica, procesa si interpreta imaginile
 primite.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/AI_ML_DP.png
	lyxscale 30
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "IA_domenii"

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Definirea domeniilor Inteligentei Artificiale
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Tipuri de Invatare Automata
\end_layout

\begin_layout Standard
Solutiile invatarii automate au nevoie de cantitati semnitifcative de date
 pentru a putea oferi raspunsuri la probleme.
 Deoarece exista multiple tipuri de probleme precum Clasificare, Detectie,
 Regresie, Reconstructie etc., formatul in care datele aferente problemei
 difera, precum si modul de abordare.
 Ergo, exista 3 tipuri importante de Invatare Automata.
\end_layout

\begin_layout Subsection
Invatarea supravegheata
\end_layout

\begin_layout Standard
\begin_inset Quotes pld
\end_inset

Învățarea supravegheată presupune învățarea unei corelări între un set de
 variabile de intrare X și o variabilă de ieșire Y, cat și aplicarea acestei
 mapări pentru a prezice ieșirile pentru date nevăzute
\begin_inset Quotes prd
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "key-2"
literal "false"

\end_inset

.
 In alte cuvinte, aceasta tehnica necesita date adnotate drept reper pentru
 rezultatele dorite in functie de datele de intrare respective.
 Pentru a asigura verosimilitatea modelului antrenat este nevoie de un set
 de date abundent si divers impreuna impreuna cu adnotarile de rigoare.
 Invatarea supravegheata este folosita preponderent in probleme de clasificare,
 detectie si regresie.
\end_layout

\begin_layout Subsection
Invatarea nesupravegheata
\end_layout

\begin_layout Standard
Spre deosebire de tipologia anterioara, Invatarea nesupravegheata are scopul
 de a determina tipare prezente in setul de date, fara a avea acces la etichetar
i ale setului de date.
 Principalele tehnici aferente invatarii nesupravegheate sunt clusterizarea
 si asocierea.
 In domeniul Invatarii profunde, una dintre cele mai populare arhitecturi
 folosite sunt Auto-Codificatoarele, cu scopul de a extrage caracteristicile
 unei imagini si a le decodifica si readuce la dimensiunile originale.
 Aceste arhitecturi sunt folosite si in domeniul procesarilor de imagini.
\end_layout

\begin_layout Subsection
Invatarea prin intarire
\end_layout

\begin_layout Standard
In aceasta tipologie este vizata recompensa interpretatorului bazata pe
 interactiunea dintre mediul si agentul ce interactioneaza cu acesta.
 Invatarea prin intarire este orientata pe rezultate finale.
 Din aceste motive, este acceptata diversitatea de solutii pe care le aplica
 agentul, atata timp cat rezultatul final este mai aroape de cel dorit.
 Astfel, aceasta metoda de invatare rezolva o problema corelarii imediate
 dintre actiunile efectualte si rezultatele intarziate ce sunt produse ce
 acestea.
 Necesitatea de a astepta pentru a observa rezultatele finale ale actiunilor
 efectuate reprezinta unul din punctele definitorii ale acestei forme de
 invatare automata, trasatura asemanatoare cu efectele actiunilor umane
 in mediul inconjurator.
\end_layout

\begin_layout Subsection
Invatarea colaborativa
\end_layout

\begin_layout Standard
Aceasta forma de invatare este o idee relativ recent adoptata in domeniul
 inteligentei artificiale, diferentiandu-se fata de celelate 3 tipuri de
 invatare importante prin faptul ca solutia adoptata este antrenata pe mai
 multe dispozitive, cu date diferite ce nu pot fi impartasite intern cu
 alte dispozitive.
 Rezultatul este un sistem decentralizat, cu un server ce agregeaza si comaseaza
 trasaturile antrenate de dispozitive, dupa care reinitializeaza aceleasi
 dispozitivele cu o solutie imbunatatita.
 Aceasta tehnica faciliteaza securitatea datelor intrucat nu exista o sursa
 centralizata a acestora.
 Structura unui asemenea sistem poate fi observata in 
\begin_inset CommandInset ref
LatexCommand ref
reference "Federated_Learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset CommandInset label
LatexCommand label
name "Federated_Learning"

\end_inset


\begin_inset Graphics
	filename Illustrations/Federated_Learning.png
	lyxscale 40
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Sistem Invatarea colaborativa
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Rețele Neuronale
\end_layout

\begin_layout Standard
Retelele Neuronale reprezinta tehnica ce sta la baza Invatarii profunde,
 fiind definita de structura sub forma de graf (noduri si muchii) ce poate
 fi asociata cu sinapsele si neuronii creierului uman.
 Din aceste motive, nodurile sunt referite ca neuroni, iar muchiile ca ponderi.
 Aspectul adanc al retelelor neuronale este denotat de prezenta multiplelor
 straturi ascunse.
 Este de mentionat faptul ca exista o pondere intre fiecare neuron din straturi
 diferite alaturate.
 O reprezentare simpla poate fi observata in 
\begin_inset CommandInset ref
LatexCommand ref
reference "Neural_Network"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset CommandInset label
LatexCommand label
name "Neural_Network"

\end_inset


\begin_inset Graphics
	filename Illustrations/NN_ex.png
	lyxscale 40
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Exemplu Retea Neuronala
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In mod normal, o retea neuronala contine un strat de intrare, un numar pozitiv
 intreg de straturi ascunse si un strat de iesire.
 Din punct de vedere matematic, ponderile sunt numere ce transforma datele
 de intrare in trecerea lor prin retea, iar neuronii reprezinta functii
 de activare.
 De asemenea, exista si parametrul numit bias, cu rolul de a ajusta suma
 ponderata la intrarea intr-un neuron.
 
\end_layout

\begin_layout Standard
Un strat des folosit in Retelele Neuronale este cel dens, ce transforma
 datele de intrare unui strat prin intermediul ponderilor si al functii
 de activare, cu formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Formula Neuron"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\sigma(Wx+b)\label{eq:Formula Neuron}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In formula de mai sus 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\hat{y}$
\end_inset

 este iesirea dintr-un neuron, 
\begin_inset Formula $\sigma$
\end_inset

 reprezinta functia de activare (neuronul), W sunt ponderile aferente neuronului
 respectiv, b este bias-ul iar x sunt datele de intrare pentru neuron.
 Pentru ca o retea sa deduca cat de departe este predictia fata de rezultatul
 dorit, este folosita o functie de cost.
 Un exemplu este o functie de cost des intalnita: Cross-Entropia Binara,
 ce are formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E=-\frac{1}{n}\sum_{k=1}^{n}((1-y_{k})ln(1-\hat{y}_{k})+y_{k}ln(\hat{y}_{k}))\label{eq:Cross-Entropy-Loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Cross-Entropy-Loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\hat{y}$
\end_inset

 este predictia retelei iar y este raspunsul corect.
 Aceasta metrica este folosita in probleme de invatare supravegheata precum
 clasificarea.
\end_layout

\begin_layout Standard
Scopul retelei este de a minimiza functia de cost pentru a produce rezultate
 cat mai corecte.
 
\end_layout

\begin_layout Subsection
Gradient Descent
\end_layout

\begin_layout Standard
Gradient Descent este o metoda de optimizare consacrata in Invatarea Profunda,
 cu scopul de a minimiza functia de cost prin actualizarea ponderilor modelului
 in directia celei mai 
\begin_inset Quotes pld
\end_inset

abrupte
\begin_inset Quotes prd
\end_inset

 pante.
 
\end_layout

\begin_layout Standard
Considerand 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Formula Neuron"
plural "false"
caps "false"
noprefix "false"

\end_inset

, putem dezvolta in:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\sigma(w_{1}x_{1}+w_{2}x_{2}+\ldots+w_{n}x_{n}b)\label{eq:Dezvoltare Formula Neuron}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Pentru a actualiza ponderile, avem nevoie de gradientul functii de eroare,
 definit prin:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\nabla E=(\frac{\delta E}{\delta w_{1}},\frac{\delta E}{\delta w_{2}},\cdots,\frac{\delta E}{\delta w_{n}},\frac{\delta E}{\delta b})\label{eq:Gradient Functia de cost}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Intr-un final, devinim o rata de invatare 
\begin_inset Formula $\alpha$
\end_inset

.
 Avand toate valorile acestea, se poate efectual un pas de Gradient Descent:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w_{k}^{*}=w_{k}-\alpha\frac{\delta E}{\delta w_{k}}\label{eq:Actualizare pondere}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
b^{*}=b-\alpha\frac{\delta E}{\delta b}\label{eq:Actualizare bias}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Feedforward
\end_layout

\begin_layout Standard
In Retele Neuronale, procesul de Feedforward este folosit pentru a transforma
 datele de intrare in date de iesire ale retelei.
 Pentru a putea defini mai bine acest concept, fie 
\begin_inset CommandInset ref
LatexCommand ref
reference "Retea Multi-Layer"
plural "false"
caps "false"
noprefix "false"

\end_inset

 cu functia de activare 
\begin_inset Formula $\sigma$
\end_inset

, ponderile 
\begin_inset Formula $W^{(i)}$
\end_inset

pentru stratul i, valoarea iesirii unui neuron 
\begin_inset Formula $h_{ij}$
\end_inset

 si predictia sistemului 
\begin_inset Formula $\hat{y}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset CommandInset label
LatexCommand label
name "Retea Multi-Layer"

\end_inset


\begin_inset Graphics
	filename Illustrations/Multi-Layer network.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Retea Neuronala cu 2 straturi ascunse
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Aplicand 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Formula Neuron"
plural "false"
caps "false"
noprefix "false"

\end_inset

pentru toate straturile, avem formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\sigma\ocircle W^{(3)}\ocircle\sigma\ocircle W^{(2)}\ocircle\sigma\ocircle W^{(1)}(x)\label{eq:Feedforward}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
cu 
\begin_inset Formula $\ocircle$
\end_inset

 drept operatorul pentru compozitia de functii.
\end_layout

\begin_layout Subsection
Backpropagation
\end_layout

\begin_layout Standard
Metoda folosita in Retele Neuronale pentru a calcula gradientii functiei
 de cost necesari metodei de optimizare Gradient Descent se numeste Backpropagat
ion.
 Aceasta are la baza folosirea Regulii Lantului pentru a calcula derivatele
 partiale 
\begin_inset Formula $\delta$
\end_inset

 ale unori functii compuse.
 Pentru 
\begin_inset Formula $A=f(x)$
\end_inset

 si 
\begin_inset Formula $B=g\ocircle f(x)$
\end_inset

, Regula Lantului defineste:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\delta B}{\delta x}=\frac{\delta B}{\delta A}\frac{\delta A}{\delta x}\label{eq:Chain Rule}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Prin folosirea recursiva a acestei reguli, Backpropagation reuseste 
\begin_inset Quotes pld
\end_inset

sa produca o expresie algebraica pentru gradientul unui scalar, cu respect
 la fiecare nod din graful computational produs de acel scalar
\begin_inset Quotes prd
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "key-3"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Framework-uri
\end_layout

\begin_layout Section
Retele Neuronale Convolutionale
\end_layout

\begin_layout Subsection
Extractorul de caracteristici (backbone)
\end_layout

\begin_layout Subsection
Tipuri de arhitecturi
\end_layout

\begin_layout Standard
Autoencoder, FPN, Multi-Level, Deformable Convolutions
\end_layout

\begin_layout Subsection
Metode de Augumentare
\end_layout

\begin_layout Section
Hiperparametrizarea unei retele
\end_layout

\begin_layout Subsection
Hiperparametrii uzuali
\end_layout

\begin_layout Subsection
Batch Norm
\end_layout

\begin_layout Section
Optimizatori Moderni
\end_layout

\begin_layout Subsection
SGD
\end_layout

\begin_layout Subsection
RMS-PROP
\end_layout

\begin_layout Subsection
Momentum
\end_layout

\begin_layout Subsection
ADAM
\end_layout

\begin_layout Subsection
Rectified ADAM
\end_layout

\begin_layout Subsection
Lookahead
\end_layout

\begin_layout Subsection
RANGER
\end_layout

\begin_layout Section
Problemele Retelelor Neuronale si Solutii
\end_layout

\begin_layout Subsection
Variance and Bias
\end_layout

\begin_layout Subsection
Explozia Gradientilor
\end_layout

\begin_layout Subsection
Functii de Activare si Moartea lui ReLU
\end_layout

\begin_layout Subsection
Regularizare
\end_layout

\begin_layout Chapter
Specificatiile proiectului
\end_layout

\begin_layout Section
Cerinte
\end_layout

\begin_layout Section
Biblioteci folosite
\end_layout

\begin_layout Section
Instalare si Utilizare
\end_layout

\begin_layout Chapter
Detalii de Implementare
\end_layout

\begin_layout Chapter
Experimente si Rezultate
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"
literal "false"

\end_inset

Cunningham, Padraig & Cord, Matthieu & Delany, Sarah.
 (2008).
 Supervised Learning.
 10.1007/978-3-540-75171-7_2.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

Goodfellow, Bengio & Courville (2016, 6.5 Back-Propagation and Other Differentiat
ion Algorithms, pp.
 200–220)
\end_layout

\end_body
\end_document
