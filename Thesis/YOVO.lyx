#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{hyperref}
\usepackage[linesnumbered,ruled,vlined,algochapter]{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}
%\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}{}}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\textit{\nouppercase{\leftmark}}}
\fancyhead[RE]{\textit{\nouppercase{\leftmark}}}

\renewcommand{\rmdefault}{ptm}

\renewenvironment{proof}{{\flushleft\itshape Rezolvare.}}{\qed}


\usepackage{babel}
  \providecommand{\proofname}{Rezolvare}
  \providecommand{\algorithmname}{Algoritmul}
  \providecommand{\definitionname}{Defini\c{t}ia}
  \providecommand{\examplename}{Exemplul}
  \providecommand{\factname}{Faptul}
  \providecommand{\lemmaname}{Lema}
  \providecommand{\corollaryname}{Corolarul}
  \providecommand{\theoremname}{Teorema}
  \providecommand{\problemname}{Exerci\c{t}iul}
\end_preamble
\use_default_options false
\begin_modules
eqs-within-sections
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-chap-bytype
\end_modules
\maintain_unincluded_children false
\language romanian
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style polish
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle empty
\bullet 1 1 34 -1
\bullet 2 2 35 -1
\bullet 3 2 7 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
YOVO: You Only Voxelize Once
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
mbox{} 
\backslash
thispagestyle{empty} 
\backslash
newpage
\backslash
setcounter{page}{3}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introducere
\end_layout

\begin_layout Section
Contextul problemei
\end_layout

\begin_layout Standard
Reconstituirea digitală a unui obiect reprezintă o problemă propusă de câteva
 decenii și este activ tratată în domeniul Viziunii Artificiale și al Graficii
 Computerizate, având ca scop final obținerea unei forme cât mai fidele
 al obiectelor reale.
 
\end_layout

\begin_layout Standard
Procedeele existente se folosesc de multiple tipuri de date pentru obținerea
 unei reconstrucții, acestea fiind obținute prin uzul de tehnologii precum:
 Camere clasice, Camere RGB-D, LiDaR, Raze X, Ultrasunete, RMN, CT etc.
 Considerând costul aferent fiecărei alternative prin hardware-ul dedicat
 necesar, se poate deduce că cea mai ieftină soluție ar necesita o simplă
 cameră RGB, cu viziune monotipică, pentru a crea imagini 2D.
 Cu toate că celelalte tipuri de date pot conduce la reproduceri volumetrice
 mai robuste, economisirea resurselor este un aspect ce concretizează interesul
 existent în soluții ce folosesc doar imagini.
\end_layout

\begin_layout Standard
Recuperarea dimensiunii a treia din poze 2D a fost țelul multor studii în
 ultimii ani.
 Prima generație de metode a tratat perspectiva geometrică din punct de
 vedere matematic, observând proiecția 2D al obiectelor tridimensionale,
 și încercând să creeze un proces reversibil.
 Soluțiile cele mai bune ale acestei abordări necesită multiple cadre ce
 capturau diferite unghiuri ale obiectelor, acestea având nevoie de o calibrare
 meticuloasă.
 A doua generație de metode a folosit o memorie ce conținea cunoștințe anterioar
e despre obiecte.
 Se poate trage o paralelă la abilitatea umana de a percepe tridimensionalitatea
 unui obiect cu ajutorul cunoștințelor precedente despre alte obiecte asemănătoa
re celui în cauză.
 Din acest punct de vedere, problema de reconstrucție al obiectelor 3D devine
 o problemă de recunoaștere.
 Considerând în prezent eficiența soluțiilor de Învățare Profundă pentru
 problemele de recunoaștere, cât și creșterea cuantumului de noi date ce
 pot fi folosite drept date de antrenare, se justifică tendința comunității
 științifice de a folosi ramuri ale Învățării Profunde precum Rețele Neuronale
 Convoluționale pentru obținerea geometriei și structurii 3D al obiectelor.
 
\end_layout

\begin_layout Section
Soluția propusa
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Exhibition YOVO.png
	lyxscale 20
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Abstraction 3D Rec"

\end_inset

Abstractie a procesului de reconstructie 3D
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Aceasta lucrare abordeaza reconstructia 3D a obiectelor din imagini 2D,
 prin intermediul retelelor neuronale.
 Deoarece natura acestei probleme face parte din domeniul Viziunii Artificiale,
 sunt folosite preponderent Retele Neuronale Convolutionale.
 
\end_layout

\begin_layout Standard
Solutia propusa se numeste YOVO: You Only Voxelize Once, intrucat se proceseaza
 o singura imagine 2D pentru estimarea unui volum voxelizat.
 Arhitectura acesteia este compusa din 3 module: 
\end_layout

\begin_layout Itemize
Autoencoder: format dintr-un Encoder ce extrage diferite trasaturi ale imaginii
 primite si un Decoder ce interpreteaza trasaturile extrase in volume voxelizate
\end_layout

\begin_layout Itemize
Agregator Contextual: realizeaza contopirea multiplelor volume deduse intr-un
 volum mai robust
\end_layout

\begin_layout Itemize
Rafinator: realizeaza cizelarea volumului unificat
\end_layout

\begin_layout Standard
Spre deosebire de soluții din aceeași familie precum 3D-R2N2
\begin_inset CommandInset citation
LatexCommand citep
key "3D-R2N2"
literal "false"

\end_inset

, care introduce aspecte recurente in reconstructia volumului prin blocuri
 LSTM și GRU 3D Convoluționale, sau Pix2Vox
\begin_inset CommandInset citation
LatexCommand citep
key "Pix2Vox"
literal "false"

\end_inset

, soluție care face tranzitia catre un Autoencoder complet convoluțional
 clasic, aceasta solutie introduce, din punct de vedere arhitectural, nivele
 adiționale de abstractie la ultimele 3 trepte ale Codificatorului, rezultând
 în 3 volume voxelizate ce capturează diferite reconstrucții ale obiectului
 real.
 Dupa trecerea prin celelalte module ale arhitecturii, rezultatul final
 este un singur volum.
 Aditional, adaugarea si integrarea procedeelor precum backbone-ul MobileNetV2,
 functii de activare Mish, regularizare DropBlock si optimizare Ranger,
 aduc rezultatele lui YOVO la nivelul SotA-ului actual pe datasetul Data3D−R2N2.
\end_layout

\begin_layout Standard
Cu ajutorul bibliotecii kaolin si matplotlib, volumele pot fi vizualizate
 intr-un mediu 3D interactiv, in care se poate analiza din orice unghi volumul
 voxelizat reconstruit.
 De asemenea, YOVO poate fi configurat sa produca videoclipuri de exibitie
 al volumelor reconstruite, sau sa le exporte in formaturi tipice aplicatilor
 grafice 3D.
\end_layout

\begin_layout Chapter
Fundamente teoretice
\end_layout

\begin_layout Section
Inteligenta Artificiala si subdomeniile ei
\end_layout

\begin_layout Standard
Odată cu creșterea popularității soluțiilor de Inteligență Artificială s-a
 creat un nivel ridicat de confuzie despre cum se definește și diferențiază
 aceasta de alte concepte precum Învățare Automată, Învățare Profundă și
 Viziune Artificială.
 Se poate observa in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "IA_domenii"
plural "false"
caps "false"
noprefix "false"

\end_inset

 structura subdomeniilor Inteligenței Artificiale.
\end_layout

\begin_layout Standard
Inteligența Artificială poate fi interpretată drept încorporarea logicii
 umane în mașini.
 Aceasta include și cele mai simple exemple de soluții implementate pe un
 calculator, cum ar fi sistemele definite de reguli condiționale.
\end_layout

\begin_layout Standard
Învatarea Automată reprezintă un subdomeniu al Inteligenței Artificiale,
 reprezentând abilitatea calculatoarelor de a 
\begin_inset Quotes pld
\end_inset

învăța
\begin_inset Quotes prd
\end_inset

 să rezolve o problemă fără a avea explicit programată fiecare instrucțiune.
 Cu cât sistemul este mai expus la un cuantum mai mare de date, cu atât
 mai mult algoritmul de învatare automată se auto-reglează.
\end_layout

\begin_layout Standard
Învățarea Profundă este la rândul sau un subdomeniu al Învățării Automate
 și descrie o tehnică de rezolvare a problemelor cu rețele neuronale, structuri
 inspirate de sistemul cerebral uman.
 Spre deosebire de celelalte tehnici alternative, învățarea profundă necesită
 mai multe resurse hardware, în funcție de profunzimea și densitatea rețelelor
 folosite.
 
\end_layout

\begin_layout Standard
Viziunea Artificială este un domeniu al Informaticii ce are ca scop dezvoltarea
 abilităților calculatoarelor de a identifica, procesa și interpreta imaginile
 primite.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/AI_ML_DP.png
	lyxscale 30
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "IA_domenii"

\end_inset

Definirea domeniilor Inteligentei Artificiale
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Tipuri de Învățare Automată
\end_layout

\begin_layout Standard
Soluțiile învățării automate au nevoie de cantități semnitifcative de date
 pentru a putea oferi răspunsuri la probleme.
 Deoarece există multiple tipuri de probleme precum Clasificare, Detecție,
 Regresie, Reconstrucție etc., formatul în care datele aferente problemei
 diferă, precum si modul de abordare.
 Ergo, există 3 tipuri importante de Învățare Automată.
\end_layout

\begin_layout Subsection
Învățarea supravegheată
\end_layout

\begin_layout Standard
\begin_inset Quotes pld
\end_inset

Învățarea supravegheată presupune învățarea unei corelări între un set de
 variabile de intrare X și o variabilă de ieșire Y, cât și aplicarea acestei
 mapări pentru a prezice ieșirile pentru date nevăzute
\begin_inset Quotes prd
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "SupervisedLearning"
literal "false"

\end_inset

.
 In alte cuvinte, această tehnică necesită date adnotate drept reper pentru
 predictiile facute.
 Pentru a asigura verosimilitatea modelului antrenat este nevoie de un set
 de date abundent și divers împreună cu adnotările de rigoare.
 Învățarea supravegheată este folosită preponderent în probleme de clasificare,
 detecție și regresie.
\end_layout

\begin_layout Subsection
Învățarea nesupravegheată
\end_layout

\begin_layout Standard
Spre deosebire de tipologia anterioară, Învățarea nesupravegheată are scopul
 de a determina tipare prezente în setul de date, fără a avea acces la etichetăr
i ale setului de date.
 Principalele tehnici aferente învățării nesupravegheate sunt clusterizarea
 și asocierea.
 În domeniul Învățării profunde, una dintre cele mai populare arhitecturi
 folosite sunt Auto-Codificatoarele, cu scopul de a extrage caracteristicile
 unei imagini și a le decodifica și readuce la dimensiunile originale.
 Aceste arhitecturi sunt folosite și în domeniul procesărilor de imagini.
\end_layout

\begin_layout Subsection
Învățarea prin întărire
\end_layout

\begin_layout Standard
În aceasta tipologie este vizată recompensa interpretatorului bazată pe
 interacțiunea dintre mediul și agentul ce interacționează cu acesta.
 Învățarea prin întărire este orientată pe rezultate finale.
 Din aceste motive, este acceptată diversitatea de soluții pe care le aplică
 agentul, atâta timp cât rezultatul final este mai aroape de cel dorit.
 Astfel, această metodă de învățare rezolvă problema corelării imediate
 dintre acțiunile efectuate și rezultatele întârziate ce sunt produse de
 acestea.
 Necesitatea de a aștepta pentru a observa rezultatele finale ale acțiunilor
 efectuate reprezintă unul din punctele definitorii ale acestei forme de
 învățare automată, trăsătura asemănătoare cu efectele acțiunilor umane
 în mediul înconjurător.
\end_layout

\begin_layout Subsection
Învățarea colaborativă
\end_layout

\begin_layout Standard
Această formă de învățare este o idee relativ recent adoptată în domeniul
 inteligenței artificiale, diferențiindu-se față de celelate 3 tipuri de
 învățare importante prin faptul că rețeaua neuronală este antrenată pe
 mai multe dispozitive, cu date diferite ce nu pot fi împărtășite intern
 cu alte dispozitive.
 Rezultatul este un sistem cu baza de date decentralizata, cu un server
 ce agreghează și comasează trăsăturile antrenate de dispozitive, după care
 reinițializează aceleași dispozitive cu o soluție îmbunătățită.
 Această tehnică facilitează securitatea datelor întrucât nu există o sursă
 centralizată a acestora.
 
\end_layout

\begin_layout Section
Rețele Neuronale
\end_layout

\begin_layout Standard
Rețelele Neuronale reprezintă tehnica ce stă la baza Învățării Profunde,
 fiind definite printr-o structură sub formă de graf (noduri și muchii)
 ce poate fi asociată cu sinapsele și neuronii creierului uman.
 Din aceste motive, nodurile sunt referite ca neuroni, iar muchiile ca ponderi
 sau parametrii.
 Aspectul adânc al rețelelor neuronale este denotat de prezența multiplelor
 straturi ascunse.
 Este de menționat faptul că există o pondere între fiecare neuron din straturi
 diferite alăturate.
 O reprezentare simplă poate fi observată în figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "Neural_Network"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/NN_ex.png
	lyxscale 40
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Neural_Network"

\end_inset

Exemplu Retea Neuronala
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
În mod normal, o rețea neuronală conține un strat de intrare, un număr întreg
 pozitiv de straturi ascunse și un strat de ieșire.
 Din punct de vedere matematic, ponderile sunt numere ce transformă datele
 de intrare în trecerea lor prin rețea, iar neuronii reprezintă funcții
 de activare.
 De asemenea, există și parametrul numit bias, cu rolul de a ajusta suma
 ponderată la intrarea într-un neuron.
 
\end_layout

\begin_layout Standard
Un strat des folosit în Rețelele Neuronale este cel dens, ce transformă
 datele de intrare al unui strat prin intermediul ponderilor și al funcțiilor
 de activare, cu formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Formula Neuron"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\sigma(Wx+b)\label{eq:Formula Neuron}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
În formula de mai sus, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\hat{y}$
\end_inset

 este ieșirea dintr-un neuron, 
\begin_inset Formula $\sigma$
\end_inset

 reprezintă funcția de activare (neuronul), 
\begin_inset Formula $W$
\end_inset

 sunt ponderile aferente neuronului respectiv, 
\begin_inset Formula $b$
\end_inset

 este bias-ul iar 
\begin_inset Formula $x$
\end_inset

 sunt datele de intrare pentru neuron.
 Pentru ca o rețea să deducă mărimea erorii față de rezultatul dorit, este
 folosită o funcție de cost.
 Un exemplu de funcție de cost des întâlnită este Cross-Entropia Binară,
 definită ca:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E=-\frac{1}{n}\sum_{k=1}^{n}((1-y_{k})ln(1-\hat{y}_{k})+y_{k}ln(\hat{y}_{k}))\label{eq:Cross-Entropy-Loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
În formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Cross-Entropy-Loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\hat{y}$
\end_inset

 este predicția rețelei, iar 
\begin_inset Formula $y$
\end_inset

 este răspunsul corect.
 Această metrică este folosită în probleme de învățare supravegheată precum
 clasificarea.
\end_layout

\begin_layout Standard
Scopul rețelei este de a minimiza funcția de cost pentru a produce rezultate
 cât mai corecte.
 
\end_layout

\begin_layout Subsection
Gradient Descent
\begin_inset CommandInset label
LatexCommand label
name "subsec:Gradient-Descent"

\end_inset


\end_layout

\begin_layout Standard
Gradient Descent este o metodă de optimizare consacrată în Învățarea Profundă,
 ce are scopul de a minimiza funcția de cost prin actualizarea ponderilor
 modelului în direcția celei mai 
\begin_inset Quotes pld
\end_inset

abrupte
\begin_inset Quotes prd
\end_inset

 pante.
 
\end_layout

\begin_layout Standard
Considerând formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Formula Neuron"
plural "false"
caps "false"
noprefix "false"

\end_inset

, putem dezvolta în:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\sigma(w_{1}x_{1}+w_{2}x_{2}+\ldots+w_{n}x_{n}b)\label{eq:Dezvoltare Formula Neuron}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Pentru a actualiza ponderile, avem nevoie de gradientul funcției de eroare,
 definit prin:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\nabla E=(\frac{\delta E}{\delta w_{1}},\frac{\delta E}{\delta w_{2}},\cdots,\frac{\delta E}{\delta w_{n}},\frac{\delta E}{\delta b})\label{eq:Gradient Functia de cost}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Într-un final, definim o rată de învățare 
\begin_inset Formula $\alpha$
\end_inset

.
 Având toate valorile acestea, se poate efectua un pas de Gradient Descent:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w_{k}^{*}=w_{k}-\alpha\frac{\delta E}{\delta w_{k}}\label{eq:Actualizare pondere}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
b^{*}=b-\alpha\frac{\delta E}{\delta b}\label{eq:Actualizare bias}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
unde 
\begin_inset Formula $w_{k}^{*}$
\end_inset

 si 
\begin_inset Formula $b^{*}$
\end_inset

 sunt noile valori ale ponderii 
\begin_inset Formula $w_{k}$
\end_inset

 și bias-ului 
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Subsection
Feedforward
\end_layout

\begin_layout Standard
În Rețele Neuronale, procesul de Feedforward este folosit pentru a transforma
 datele de intrare în date de ieșire ale rețelei.
 Pentru a putea defini mai bine acest concept, fie structura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Retea Multi-Layer"
plural "false"
caps "false"
noprefix "false"

\end_inset

 cu funcția de activare 
\begin_inset Formula $\sigma$
\end_inset

, ponderile 
\begin_inset Formula $W^{(i)}$
\end_inset

pentru stratul 
\begin_inset Formula $i$
\end_inset

 și predicția sistemului 
\begin_inset Formula $\hat{y}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Multi-Layer network.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Retea Multi-Layer"

\end_inset

Retea Neuronala cu multiple straturi ascunse
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Aplicând formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Formula Neuron"
plural "false"
caps "false"
noprefix "false"

\end_inset

 pentru toate straturile din figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Retea Multi-Layer"
plural "false"
caps "false"
noprefix "false"

\end_inset

, avem formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\sigma\ocircle W^{(3)}\ocircle\sigma\ocircle W^{(2)}\ocircle\sigma\ocircle W^{(1)}(x)\label{eq:Feedforward}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
cu 
\begin_inset Formula $\ocircle$
\end_inset

 drept operatorul pentru compoziția de funcții.
\end_layout

\begin_layout Subsection
Backpropagation
\end_layout

\begin_layout Standard
Metoda folosită în Rețele Neuronale pentru a calcula gradienții funcției
 de cost necesari metodelor de optimizare se numește Backpropagation.
 Aceasta are la bază folosirea Regulii Lanțului pentru a calcula derivatele
 parțiale 
\begin_inset Formula $\delta$
\end_inset

 ale unor funcții compuse.
 
\end_layout

\begin_layout Standard
Pentru 
\begin_inset Formula $A=f(x)$
\end_inset

 și 
\begin_inset Formula $B=g\ocircle f(x)$
\end_inset

, Regula Lanțului definește:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\delta B}{\delta x}=\frac{\delta B}{\delta A}\frac{\delta A}{\delta x}\label{eq:Chain Rule}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Prin folosirea recursivă a acestei reguli, Backpropagation reușește 
\begin_inset Quotes pld
\end_inset

să producă o expresie algebrică pentru gradientul unui scalar, cu respect
 la fiecare nod din graful computațional produs de acel scalar
\begin_inset Quotes prd
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "Backpropagation"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Framework-uri
\end_layout

\begin_layout Standard
Un framework pentru Învățarea Profundă este o bibliotecă ce simplifică procesul
 de creare al unui model prin folosirea unor sintaxe mai inteligibile.
 Acestea sunt create pentru a interpreta și optimiza interacțiunea dintre
 utilizator și dispozitivele de calcul precum plăci video sau procesoare.
 De exemplu, un framework va integra transformările și operațiile cu matrici,
 va face posibilă paralelizarea procesului computațional sau va ușura vizualizar
ea și înregistrarea de parametri.
 
\end_layout

\begin_layout Standard
Două dintre cele mai populare framework-uri sunt:
\end_layout

\begin_layout Standard
1.
 
\bar under
Tensorflow
\bar default

\begin_inset CommandInset citation
LatexCommand citep
key "Tensorflow"
literal "false"

\end_inset

: oferă instrumente utile precum Tensorboard pentru vizualizarea clară a
 pipeline-ului unui model, permite RT-serving, un proces de optimizare semi-auto
mată al modelelor prin tehnici precum prunizare, cuantizare, împărtășire
 de ponderi.
 Din aceste motive, Tensorflow aduce avantaje în crearea de modele pentru
 producție, dar are dezavantajul de a prezenta o sintaxă mai dificilă și
 complexă.
\end_layout

\begin_layout Standard
2.
 
\bar under
Pytorch
\bar default

\begin_inset CommandInset citation
LatexCommand citep
key "Pytorch"
literal "false"

\end_inset

: bazat pe biblioteca Torch, oferă claritate și ușurință în folosire, având
 tendințe 
\begin_inset Quotes pld
\end_inset

Pythonice".
 De asemenea, este un framework ce simplifică procesul depanării de cod,
 având avantajul unei sintaxe ușoare, experimentele fiind ușor de realizat.
 Din aceste motive, comunitatea academică tinde să folosească acest framework
 la scara largă.
\end_layout

\begin_layout Section
Rețele Neuronale Convoluționale
\end_layout

\begin_layout Standard
\begin_inset Quotes pld
\end_inset

Rețelele Neuronale Convoluționale sunt un tip de învățare profundă pentru
 a procesa date ce au structura în formă de grilă, precum imaginile, ce
 este inspirată de organizarea cortexului vizual și este proiectată să învețe
 în mod automat și adaptabil ierarhii spațiale de caracteristici, de la
 tipare de nivel mic la cele de nivel mai mare.
 Rețelele Neuronale Convoluționale sunt un crez matematic ce este compus
 din trei tipuri de straturi (sau blocuri): convoluție, pooling și strat
 dens.
\begin_inset Quotes prd
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "ReteleConvDef"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Convoluția este o operație matematică care, în domeniul Viziunii Artificiale
 și a Învățării profunde este folosită pentru a aplica filtre asupra imaginilor.
 În urma aplicării unui set de filtre, rezultatul este un ansamblu de hărți
 de caracteristici, compus din canale.
 Acest ansamblu se mai numește strat.
 Din punct de vedere matematic, acestea sunt reprezentate în formă de tensori,
 ce reprezintă o formă generalizată a unei matrici.
 Tensorii pot fi atat 
\begin_inset Formula $0$
\end_inset

-dimensionali (un singur număr), cât și pluridimensionali.
\end_layout

\begin_layout Subsection
Convoluții Multidimensionale și Terminologii
\end_layout

\begin_layout Standard
Un strat conține o multitudine de canale.
 Kernel-ul este o matrice de ponderi ce se înmulțește cu porțiuni ale stratului
 de intrare, valorile rezultate fiind însumate într-o singură valoare finală
 reprezentativă pentru porțiunea respectivă.
 Un filtru de convoluție reprezintă un ansamblu de kernel-uri stivuite.
 Pasul unei convoluții reprezintă din câte în câte puncte se aplică filtrele.
 Folosirea unui pas mai mare de 1 duce la reducerea exponențială a dimensionalit
ății.
 O reprezentare a diferitelor tipuri de kerneluri este prezentată în figura
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kerneluri Multidimensionale"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/kernels.png
	lyxscale 70
	scale 85

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kerneluri Multidimensionale"

\end_inset

Kernel-uri de convolutie
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Pentru aplicarea unei convolutii 2D, un kernel este aplicat fiecarui canal,
 in adancimea stratului, iar rezultatele sunt adunate si agregate intr-un
 nou canal.
 Numarul de kerneluri din filtru reprezinta numarul de canale pe care il
 va rezulta convolutia.
 Convolutiile 2D pot deplasa filtrele doar pe inaltimea si latimea stratului.
 Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Convolutie normala"
plural "false"
caps "false"
noprefix "false"

\end_inset

 prezinta o convolutie simpla, cu un kernel de 2x2 si 6 canale de iesire.
 Convolutiile 3D contin kernel-uri 3D, ce se aplica independent in adancimea
 stratului de intrare.
 Astfel, convolutiile 3D deplaseaza filtrele atat pe inaltimea si latimea
 stratului, cat si pe adancimea acestuia.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Convolutie Normala.png
	lyxscale 50
	scale 55

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Convolutie normala"

\end_inset

Convolutie 2D cu 6 canale de iesire
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Operatia opusa convolutiei, din punct de vedere al dimensionalitatii, este
 convolutia transpusa.
 Daca o convolutie clasica are ca rezultat un strat cu dimensiuni egale
 sau mai mici decat cele ale stratului de intrare, convolutia transpusa
 are scopul de a mari dimensionalitate, prin adaugarea de padding sau prin
 dilatarea convolutilor.
 
\end_layout

\begin_layout Standard
In aceasta lucrare ne vom referi la reducerea sau cresterea dimensionalitatii
 drept downsampling, respectiv upsampling.
\end_layout

\begin_layout Subsection
Convolutii separabile
\end_layout

\begin_layout Standard
Un alt concept folosit in diverse arhitecturi este eficientizarea operatiei
 de convolutie prin convolutii separabile
\begin_inset CommandInset citation
LatexCommand citep
key "Separable_Conv"
literal "false"

\end_inset

.
 Acestea impart procesul unei convolutii intr-o convolutie depthwise, ce
 doar reduce dimensiunile stratului de intrare, si alta pointwise, ce mareste
 numarul de canale.
 Rezultatul este un numar substantial redus de operatii.
 Un exemplu este prezentat in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Convolutie separabila"
plural "false"
caps "false"
noprefix "false"

\end_inset

 drept alternativa la convolutia prezenta in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Convolutie normala"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Convolutie_Separabila.png
	lyxscale 50
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Convolutie separabila"

\end_inset

Convolutie separabila
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In cazul din figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Convolutie normala"
plural "false"
caps "false"
noprefix "false"

\end_inset

, pe stratul de intrare se aplica filtre de 
\begin_inset Formula $2\vartimes2\vartimes3$
\end_inset

, de 
\begin_inset Formula $4\vartimes4$
\end_inset

 ori pentru a avea un singur strat de iesire.
 Se aplica de 
\begin_inset Formula $6$
\end_inset

 ori pasii precedenti, dimensiunile rezultatului final fiind 
\begin_inset Formula $4\vartimes4\vartimes3$
\end_inset

.
 In total avem 
\begin_inset Formula $2*2*3*4*4*6=1152$
\end_inset

 de inmultiri.
 
\end_layout

\begin_layout Standard
Aplicand convolutia separabila prezentata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Convolutie separabila"
plural "false"
caps "false"
noprefix "false"

\end_inset

, pe stratul de intrare se folosesc 3 filtre de 
\begin_inset Formula $2\vartimes2\vartimes1$
\end_inset

, de 
\begin_inset Formula $4\vartimes4$
\end_inset

 ori.
 Acest pas se numeste convolutie depthwise si se efectueaza o singura data.
 Numarul de operatii pentru aceasta tehnica este 
\begin_inset Formula $3*2*2*1*4*4=192$
\end_inset

 de inmultiri.
 
\end_layout

\begin_layout Standard
Rezultatului intermediar dupa convolutia depthwise ii este aplicata o convolutie
 pointwise, cu un filtru de 
\begin_inset Formula $1\vartimes1\vartimes3$
\end_inset

, de 
\begin_inset Formula $4\vartimes4$
\end_inset

 ori.
 Acest pas este repetat de 6 ori.
 In total, convolutia pointwise prezinta 
\begin_inset Formula $1*1*3*4*4*6=288$
\end_inset

 de inmultiri.
 Astfel, convolutia separabila are in total 
\begin_inset Formula $192+288=480$
\end_inset

 de inmultiri fata de cele 
\begin_inset Formula $1152$
\end_inset

 ale unei convolutii clasice.
 Aceasta diferenta devine mai semnificativa la date de intrare mari.
 
\end_layout

\begin_layout Standard
Dezavantajul convolutilor separabile este reducerea numarului de ponderi
 din convolutie, posibil limitand capacitatea de invatare fata de o convolutie
 normala.
 Cu toate acestea, eficienta rezultata din convolutiile separabile compenseaza
 pentru potentialele aspecte negative.
\end_layout

\begin_layout Subsection
Blocuri Reziduale
\end_layout

\begin_layout Standard
Un bloc rezidual clasic 
\begin_inset CommandInset citation
LatexCommand citep
key "Residual 1"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "Residual 2"
literal "false"

\end_inset

 este format dintr-o convolutie 2D cu kernel 
\begin_inset Formula $1\vartimes1$
\end_inset

 ce reduce numarul de filtre, o a doua convolutie 2D cu kernel 
\begin_inset Formula $3\vartimes3$
\end_inset

 ce nu reduce dimensionalitatea, urmat de o a treia convolutie 2D cu kernel
 
\begin_inset Formula $1\vartimes1$
\end_inset

 ce readuce numarul de filtre la cel initial, elementele iesirii ultimei
 convolutii fiind adunate cu elementele intrarii din din prima convolutie.
 Aceasta conexiune dintre Input si Output se numeste skip connection si
 se poate aplica doar cand dimensiunile acestora sunt identice.
 Toate convolutile au ReLU (descris in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Functii-de-Activare"
plural "false"
caps "false"
noprefix "false"

\end_inset

) ca functie de activare.
 Blocul rezidual este reprezentat in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bloc-Rezidual"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Aceste blocuri au fost create pentru a combate problema gradientilor cu
 valori foarte mici.
 Deoarece acest bloc nu adauga un nou nivel de abstractie, ci doar rafineaza
 informatia, stivuirea mai multor blocuri reziduale pot spori puterea computatio
nala, astfel modelul reusind sa invete mai bine caracteristicile existente.
 Acest concept poate fi vazut ca si cum iesirea din aceasta structura poate
 fi influentata doar de caracteristicile intermediare, dar nu poate fi anulata
 de acestea.
 Denumirea 
\begin_inset Quotes pld
\end_inset

reziduala
\begin_inset Quotes prd
\end_inset

 provine din faptul ca iesirea din acest bloc este egala cu intrarea plus
 informatia reziduala captata.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Residual.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Bloc-Rezidual"

\end_inset

Bloc Rezidual
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Blocuri Reziduale Inversate cu Linear Bottleneck
\end_layout

\begin_layout Standard
Introduse in 
\begin_inset CommandInset citation
LatexCommand citep
key "Inverted Residuals and Linear Bottlenecks"
literal "false"

\end_inset

, acestea au pornit de la premisa ca hartile de caracteristici pot fi codificate
 in straturi cu dimensiune redusa (cu mai putine canale) si ca activarile
 non-liniare (e.g.
 ReLU) provoaca pierderi de informatie, chiar daca au abilitatea de reprezentare
 a complexitatii datelor.
 In aceasta structura sunt folosite activari liniare 
\begin_inset Formula $f(x)=x$
\end_inset

 (functia identitate) si activari non-liniare ReLU6, ce sunt defapt activari
 ReLU cu limita superioara 6.
 
\end_layout

\begin_layout Standard
Acest bloc primeste ca input un tensor si aplica in prima faza o convolutie
 pointwise ce creste numarul de canale.
 Aeasta are ca scop pregatirea tensorului pentru operatii non-liniare, ce
 necesita o crestere in dimensiunea tensorului.
 Dupa o activare ReLU6, este aplicata a doua convolutie: una depthwise cu
 kernel de 
\begin_inset Formula $3\vartimes3$
\end_inset

, urmata de o alta activare ReLU6, cu rolul de a filtra dimensiunile marite
 ale tensorului.
 A treia convolutie este una pointwise ce proiecteaza numarul mare de canale
 intr-o dimensiune redusa, egala cu cea a datelor de intrare.
 
\end_layout

\begin_layout Standard
Intrucat dupa a treia convolutie am revenit in spatiul redus din punct de
 vedere dimensional, aplicarea unei activari non-liniare ar cauza prea multa
 pierdere de informatii.
 In consecinta, se foloseste activarea liniara 
\begin_inset Formula $f(x)=x$
\end_inset

.
 Daca nu se doreste reducerea dimensiunilor principale ale hartilor de caracteri
stici (lungimea si latimea), este efectuat un skip connection dintre input
 si output.
 In caz contrar, nu este aplicata o asemenea conexiune.
 Structura acestui bloc este prezentata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bloc Rezidual Inversat cu Linear Bottleneck"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Inverted Residual.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Bloc Rezidual Inversat cu Linear Bottleneck"

\end_inset

Block Rezidual Inversat cu Linear Bottleneck
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Alte tipuri de straturi des folosite
\end_layout

\begin_layout Standard
Unul dintre cele mai comune straturi folosite pentru reducerea dimensionalitatii
 este stratul de pooling.
 Acesta aplica la randul lui un filtru de kernel-uri pe datele de intrare.
 In functie de tipul stratului de pooling, aceste kerneluri pot comasa datele
 de intrare dupa valorile medii, sau dupa valoriile maximale.
\end_layout

\begin_layout Standard
O caracteristica ce poate fi asociata unei multimi de straturi este padding-ul,
 ce extinde dimensiunile acestora.
 Extinderea se poate realiza cu valori nule sau cu media valorilor vecine.
 Aplicarea padding-ului poate evita reducerea dimensionalitatii la operatii
 cu kernel.
\end_layout

\begin_layout Standard
Un alt strat des folosit in Retelele Neuronale Convolutionale este cel de
 Batch Normalization, ce are ca scop cresterea stabilitatii.
 Acesta este aplicat dupa functia de activare si functioneaza prin normalizarea
 valorilor de intrare 
\begin_inset Formula $x$
\end_inset

.
 Valorile normalizate 
\begin_inset Formula $\hat{x}$
\end_inset

 sunt obtinute prin scaderea mediei intrarilor si impartirea la deviatia
 standard a acestora.
 Normalizarea are efectul de a preveni situatiile in care o activare produce
 valori foarte mari sau foarte mici.
 Pentru a evita potentialele situatii destabilizante ale acestei normalizari,
 se adauga doi parametrii antrenabili: 
\begin_inset Formula $\gamma-(gamma)$
\end_inset

 pentru deviatia standard si 
\begin_inset Formula $\beta-(beta)$
\end_inset

 pentru medie.
 Formula simplificata a stratului de Batch Normalization este:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y=\gamma*\hat{x}+\beta\label{eq:Batch Normalization}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Astfel, in cazul in care un optimizator trebuie sa denormalizeze valorile,
 se vor modifica doar acesti parametri.
 De asemenea, folosirea acestui strat anuleaza valorile de bias din stratul
 respectiv, acestea fiind echivalente cu 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
Un alt tip de straturi folosite sunt cele de regularizare.
 Acestea sunt definite de metoda de regularizare aleasa, si au rolul de
 a evita cazurile in care o retea memoreaza setul de date si nu poate generaliza
 pe date noi.
\end_layout

\begin_layout Subsection
Backbone
\end_layout

\begin_layout Standard
Backbone-ul are rolul de a extrage caracteristicile dintr-o imagine si este
 structurat prin suprapunerea de straturi de convolutie, pooling si alte
 operatii.
 Acesta este de obicei folosit pentru transferul de cunostinte, intrucat
 un backbone preantrenat sa detecteze caracteristici generale poate imbunatati
 considerabil viteza de invatare a unei retele.
 Printre cele mai populare arhitecturi se numara AlexNet, VGG si ResNet.
\end_layout

\begin_layout Subsection
Arhitecturi folosite in Retele Neuronale Convolutionale
\end_layout

\begin_layout Standard
Primele arhitecturi folosite in domeniu aveau structura secventiala.
 Aparitia solutiilor moderne a adus o multitudine de variatii arhitecturale
 ce reusesc sa extraga informatia captata in imagini si sa detecteze caracterist
ici atat mai generale, cat si mai concrete.
 
\end_layout

\begin_layout Standard
Una dintre aceste structuri este Autoencoder-ul
\begin_inset CommandInset citation
LatexCommand citep
key "AutoEncoder"
literal "false"

\end_inset

, format dintr-un Encoder, ce reduce dimensionalitatea input-ului si are
 rolul de a codifica o reprezentare a acestuia.
 A doua parte a acestei structuri este Decoderul, ce creeaza o reproducere
 cat mai fidela a input-ului bazata pe rezultatul Encoder-ului.
 Astfel, caracteristicile extrase isi pastreaza aceeasi dimensionalitate
 ca si imaginea originala.
 O variatiune a acestuia este U-Net
\begin_inset CommandInset citation
LatexCommand citep
key "UNet"
literal "false"

\end_inset

, ce adauga skip connections intre nivelele Encoder-ului si Decoder-ului,
 pentru a pastra caracteristici pierdute in procesul de reducere.
\end_layout

\begin_layout Standard
O alta arhitectura este Feature Pyramid Network
\begin_inset CommandInset citation
LatexCommand citep
key "FPN"
literal "false"

\end_inset

, ce prezinta 2 structuri piramidale paralele.
 Un aspect important al acestei structuri este transmiterea caracteristicilor
 prin skip connections intre nivele paralele ale piramidelor, cat si predictiile
 multi-level ce permit procesarea caracteristicilor la diferite nivele de
 complexitate.
\end_layout

\begin_layout Subsection
Metode de Augmentare
\end_layout

\begin_layout Standard
Pentru ca o retea neuronala sa aibe o performanta semnificativa este nevoie
 de un set de date cat mai mare si divers, astfel permitand modelului produs
 sa generalizeze pe date noi.
 Cu toate acestea, nu toate seturile de date prezinta varietate, permitand
 utilizatorului sa aplice metode de augmentare pentru a spori diversitatea.
 Printre tehnicile de augmentare se numara:
\end_layout

\begin_layout Enumerate
Decuparea - se elimina mai multe randuri de pixeli.
 Decuparea poate fi centrata, laterala sau aletorie.
\end_layout

\begin_layout Enumerate
Redimensionarea - modificarea dimensiunilor imaginii afectand marimea obiectelor
 prezente.
 Aceasta augmentare poate produce pierdere de informatii la comasarea pixelilor.
\end_layout

\begin_layout Enumerate
Variatia fundalului - pentru seturile de date cu fundalul monocrom si usor
 de izolat, se pot folosi game diferite de culori.
 Aceasta metoda de augmentare este folosita si pentru datele de intrare
 compozite, ce se formeaza in timpul executiei programului.
\end_layout

\begin_layout Enumerate
Bruiaj cromatic - modificarea luminozitatii, contrastului si saturatiei
 unei imagini
\end_layout

\begin_layout Enumerate
Inducerea de noise in imagine - este variatia aleatorie a culorii pixelilor
 unei imagini.
 Acest noise poate varia in functie de functia densitatii de probabiltate.
\end_layout

\begin_layout Enumerate
Normalizare - o imagine contine informatii ale pixelilor pe cele 3 canale
 RGB, cu valori intre 0-255.
 Normalizarea aduce acele valori intr-un interval potrivit retelei in cauza.
 In domeniul Retelelor Neuronale, normalizarea si standardizarea denota
 acelasi proces, termenii fiind folositi interschimbabil.
\end_layout

\begin_layout Enumerate
Rotatie - rotirea unei imagini cu un numar de grade variabil.
\end_layout

\begin_layout Enumerate
Flip - imaginea este intoarsa 
\begin_inset Formula $180$
\end_inset

 de grade in jurul axei orizontale sau verticale.
\end_layout

\begin_layout Enumerate
Permutarea canalelor de culoare - se interschimba valorile canalelor de
 culoare RGB (rosu-verde-albastru)
\end_layout

\begin_layout Section
Aspectele unei retele
\end_layout

\begin_layout Standard
Pentru a produce un model capabil de performante satisfacatoare, trebuie
 definit procesul de antrenare, cel de validare in timpul antrenarii, cat
 si procesul de testare si evaluare al performantei.
\end_layout

\begin_layout Subsection
Hiperparametrii
\end_layout

\begin_layout Standard
In aplicatiile de Invatare Profunda, exista parametrii simpli, ce contin
 informatia invatata de retea.
 Acesti parametrii pot fi antrenabili si sunt reprezentati de ponderi, bias,
 gamma si beta (Batch Normalization), etc.
 
\end_layout

\begin_layout Standard
De asemenea, exista si hiperparametrii, ce definesc procesul de antrenare
 si sunt setati inaintea inceperii acestuia.
 Printre cei mai consacrati hiperparametrii se numara:
\end_layout

\begin_layout Itemize
Rata de invatare (
\begin_inset Formula $lr$
\end_inset

) - aplicata in tehnica de optimizare aleasa.
\end_layout

\begin_layout Itemize
Functiile de Activare - guverneaza cum sunt procesate sau filtrate ponderile.
\end_layout

\begin_layout Itemize
Parametrii Optimizatorului - initializeaza si seteaza anumite limite in
 procesul de optimizare.
 Un optimizator are scopul de a micsora functia de cost.
\end_layout

\begin_layout Itemize
Marimea lotului (
\begin_inset Formula $batch\text{\_}size$
\end_inset

) - deoarece este ineficient si costisitor sa parcurgem intreg setul de
 date ca sa efectuam un pas de optimizare, acesta este divizat in loturi,
 asigurand o oarecare invatare a cazurilor neobisnuite care altfel nu ar
 fi avut niciun impact semnificativ.
\end_layout

\begin_layout Itemize
Numarul de epoci - o parcurgere a intregului set de date este definit ca
 o epoca.
 Retelele neuronale necesita multiple parcurgeri pentru a deveni eficiente.
\end_layout

\begin_layout Itemize
Parametrii arhitecturali - numarul de blocuri structurale folosite in arhitectur
a sau numarul de straturi ascunse
\end_layout

\begin_layout Standard
Procesul de alegere a valorilor optime se numeste Reglarea Hiperparametrilor.
 In mod normal acestia sunt alesi arbitrar, sau determinati empiric.
 Cu toate acestea, in ultimii ani au aparut metode de automatizare a acestui
 procesului de reglare hiperparametrica, precum AutoML
\begin_inset CommandInset citation
LatexCommand citep
key "AutoML"
literal "false"

\end_inset

.
 Aceste metode reprezinta o solutie accesibila pentru antrenarea si lansarea
 in productie a aplicatilor cu retele neuronale, dar prezinta limitari in
 domeniul academic, ce are ca scop depasirea solutiilor State-of-the-Art.
\end_layout

\begin_layout Subsection
Functii de Activare
\begin_inset CommandInset label
LatexCommand label
name "subsec:Functii-de-Activare"

\end_inset


\end_layout

\begin_layout Standard
Functia de activare reprezinta o ecuatie matematica ce filtreaza valorile
 de intrare.
 Din punct de vedere matematic, pentru aplicarea algoritmului Gradient Descent,
 acestea sunt de preferat sa fie diferentiabile pe tot domeniul de valori.
 Printre cele mai reprezentative functii de activare se numara:
\end_layout

\begin_layout Enumerate
Sigmoid - folosita in general pentru clasificarea binara, intrucat rezultatul
 acestei functii are valori intre 0 si 1.
 Reprezinta un caz particular al functiei Softmax.
 Pentru ambele functii, componentele au suma 1.
\begin_inset Formula 
\begin{equation}
Sigmoid(x)=\frac{1}{1+e^{-x}}\label{eq:Sigmoid}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
Softmax(x_{i})=\frac{e^{x_{i}}}{\sum_{j=1}^{N}e^{x_{j}}}\label{eq:Softmax}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
ReLU (Rectified Linear Unit) - introdus in solutiile de invatare profunda
 de 
\begin_inset CommandInset citation
LatexCommand citep
key "ReLU"
literal "false"

\end_inset

, a devenit cea mai populara functie de activare, prin rezolvarea partiala
 a problemei disparitiei gradientului si rapiditatii computationale.
 Aceasta functie este diferentiabila in toate punctele exceptand 
\begin_inset Formula $x=0$
\end_inset

, intrucat o derivata are valoarea 1 iar cealalta are valoarea 0.
 Pentru a evita posibile complicatii, in general este convenit ca derivata
 in 
\begin_inset Formula $x=0$
\end_inset

 este cea de pe a doua ramura.
 Exista variatii de ReLU in care sunt impuse limite superioare (e.g.
 ReLU6).
\begin_inset Formula 
\begin{equation}
ReLU(x)=\begin{cases}
x, & x>0\\
0, & otherwise
\end{cases}\label{eq:ReLU}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
Leaky-ReLU (Leaky Rectified Linear Unit) - introdus de 
\begin_inset CommandInset citation
LatexCommand citep
key "Leaky-ReLU"
literal "false"

\end_inset

, aduce o solutie la problemele in care activarile ReLU 
\begin_inset Quotes pld
\end_inset

mor
\begin_inset Quotes prd
\end_inset

, adica tind spre 0.
 Acest lucru se realizeaza prin tratarea valorilor negative cu o pondere
 mica, valori ce in mod normal nu ar fi tratate de ReLU.
\begin_inset Formula 
\begin{equation}
LeakyReLU(x)=\begin{cases}
x, & x>0\\
0.01x, & otherwise
\end{cases}\label{eq:Leaky-ReLU}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
ELU (Exponential Linear Unit) - introdus de 
\begin_inset CommandInset citation
LatexCommand citep
key "ELU"
literal "false"

\end_inset

 si spre deosebire de activarile precedente, aceasta functie realizeaza
 mai bine tranzitia de la valori pozitive la valori negative, adaugand un
 hiperparametru 
\begin_inset Formula $\alpha$
\end_inset

, cu valoarea de baza 1.
 
\begin_inset Formula 
\begin{equation}
ELU(x)=\begin{cases}
x, & x>0\\
\alpha(e^{x}-1), & otherwise
\end{cases}\label{eq:ELU}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
Swish - creata de 
\begin_inset CommandInset citation
LatexCommand citep
key "Swish"
literal "false"

\end_inset

, trateaza intr-o maniera mai buna problemele ce apar in ReLU, diferentiandu-se
 de celelalte activari prin faptul ca functia este diferentiabila in toate
 punctele si nu este monotona, avand limita inferioara in punctul de inflexiune
 al functiei.
 De asemenea, functia nu este limitata superior si prezinta un parametru
 
\begin_inset Formula $\beta$
\end_inset

, ce poate fi constant sau antrenabil.
 Cand 
\begin_inset Formula $\beta$
\end_inset

 este 0, functia devine liniara, iar cand acesta tinde spre 
\begin_inset Formula $\infty$
\end_inset

, functia devine ReLU.
 Valoarea de baza al lui 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\beta$
\end_inset

 este 1.
\begin_inset Formula 
\begin{equation}
Swish(x)=x*Sigmoid(\beta x)\label{eq:Swish}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
Mish - descrisa de 
\begin_inset CommandInset citation
LatexCommand citep
key "Mish"
literal "false"

\end_inset

 si inspirata de 
\begin_inset CommandInset citation
LatexCommand citep
key "Swish"
literal "false"

\end_inset

, aceasta functie incearca sa rezolve aceleasi probleme dar reuseste sa
 evite mai eficient saturarea functiei in valorile negative, problema existenta
 in functiile prezentate.
 De asemenea, este diferentiabila in toate punctele.
 Fiind relativ recent descoperita, analize comparative sunt realizate si
 in prezent.
\begin_inset Formula 
\begin{equation}
Mish(x)=x*tanh(ln(1+e^{x}))\label{eq:Mish}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Ecuatiile enuntate mai sus sunt reprezentate vizual in figurile 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Sigmoid-ReLU"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:LReLU si ELU"
plural "false"
caps "false"
noprefix "false"

\end_inset

 si 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Swish-Mish"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Sigmoid.png
	lyxscale 40
	scale 26

\end_inset

 
\begin_inset space \hfill{}
\end_inset

 
\begin_inset Graphics
	filename Illustrations/ReLU.png
	lyxscale 40
	scale 26

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Sigmoid-ReLU"

\end_inset

Sigmoid (stanga) si ReLU(dreapta)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Leaky-ReLU.png
	lyxscale 40
	scale 26

\end_inset

 
\begin_inset space \hfill{}
\end_inset

 
\begin_inset Graphics
	filename Illustrations/ELU.png
	lyxscale 40
	scale 26

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:LReLU si ELU"

\end_inset

Leaky-ReLU (stanga) si ELU(dreapta)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Swish.png
	lyxscale 40
	scale 26

\end_inset

 
\begin_inset space \hfill{}
\end_inset

 
\begin_inset Graphics
	filename Illustrations/Mish.png
	lyxscale 40
	scale 26

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Swish-Mish"

\end_inset

Swish (stanga) si Mish(dreapta)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Functii de Cost
\end_layout

\begin_layout Standard
In domeniul Invatarii Profunde, functiile de cost sunt folosite in procesul
 de optimizare pentru a calcula eroarea modelului, fapt pentru care valoarea
 acestora este referita drept 
\begin_inset Quotes pld
\end_inset

eroarea modelului
\begin_inset Quotes prd
\end_inset

.
 Cea folosita in aceasta lucrare se numeste Cross-Entropie Binara Voxel-Unitara.
 
\end_layout

\begin_layout Standard
Un voxel reprezinta cea mai minuscula unitate ce poate fi recunoscuta intr-un
 sistem, echivalentul tridimensional al unui pixel.
\end_layout

\begin_layout Standard
Considerand o pozitie in spatiul 3D cu un volum echivalent unui voxel, avem
 doua evenimente posibile: exista sau nu exista un voxel in pozitia respectiva.
 Mai departe, definim ocupanta drept probabilitatea unei pozitii de a fi
 ocupata cu un voxel.
 Avand un set de valori probabilistice pentru ocupante, un set de evenimente
 si ambele seturi pentru toate punctele din spatiul 3D existent, Cross-Entropia
 denota cat este de probabil ca acele evenimente sa se intample bazandu-se
 pe probabilitatea evenimentelor.
 Daca este foarte probabil, avem o Cross-Entropie mica.
 In caz contrar, Cross-Entropia este mare.
 Formula pentru Costul Cross-Entropic Binar Voxel-Unitar este:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L=-\frac{1}{N}\sum_{k=1}^{N}((1-gt_{k})ln(1-o_{k})+gt_{k}ln(o_{k}))\label{eq:Cost Cross-Entropic Binar Voxel-Unitar}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
unde 
\begin_inset Formula $N$
\end_inset

 este numarul de voxeli din volumul real, 
\begin_inset Formula $o_{k}$
\end_inset

 reprezinta ocupanta pentru pozitia 
\begin_inset Formula $k$
\end_inset

 iar 
\begin_inset Formula $gt_{k}$
\end_inset

este valoarea reala pentru pozitia 
\begin_inset Formula $k$
\end_inset

.
 Din cauza naturii binare a lui 
\begin_inset Formula $gt_{k}$
\end_inset

, termenul din interiorul sumei poate fi 
\begin_inset Formula $ln(1-o_{k})$
\end_inset

 sau 
\begin_inset Formula $ln(o_{k})$
\end_inset

.
 In consecinta, putem defini Cross-Entropia din punct de vedere matematic
 drept negativul sumei logaritmilor naturali ale probabilitatilor aferente
 evenimentelor reale.
\end_layout

\begin_layout Subsection
Metrici
\end_layout

\begin_layout Standard
In scopul reconstructiei 3D a obiectelor cu voxeli, metrica uzuala este
 3D-Intersection-over-Union, sau 3D-IoU.
 Formula acesteia este:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
3DIoU=\frac{V_{reconstruit}\cap V_{real}}{V_{reconstruit}\cup V_{real}}\label{eq:3D-IoU}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
unde 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $V_{reconstruit}$
\end_inset

 si 
\begin_inset Formula $V_{real}$
\end_inset

 este volumul prezis de retea, respectiv volumul real pentru obiect.
 Un exemplu poate fi observat in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3D-IoU"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/3D-IoU.png
	lyxscale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:3D-IoU"

\end_inset

Exemplu de 3D-IoU a 2 cuburi de 
\begin_inset Formula $5\vartimes5\vartimes3$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Optimizatori Moderni
\end_layout

\begin_layout Standard
In domeniul Invatarii Profunde, optimizatorul este algoritmul ce actualizeaza
 paramentrii antrenabili ai unei retele neuronale, cu scopul de a ajunge
 in minimul global al functiei de cost.
 Alegerea optimizatorului si ajustarea hiperparametrilor acestuia este un
 aspect foarte important, intrucat este dorita trecerea peste punctele de
 minim local ale functiei, cat si o convergenta cat mai rapida la valoarea
 dorita.
\end_layout

\begin_layout Subsection
Cele 3 variatii ale Gradient Descent
\end_layout

\begin_layout Standard
Concretizata in domeniul Invatarii Automate de 
\begin_inset CommandInset citation
LatexCommand citep
key "SGD"
literal "false"

\end_inset

, Stochastic Gradiend Descent, sau SGD, este o aproximare stohastica a metodei
 de optimizare Gradient Descent.
 Pentru fiecare exemplu din setul de date, SGD calculeaza gradientul erorii
 si actualizeaza ponderile.
 Aspectul stohastic provine din faptul ca aceasta metoda estimeaza valoarea
 gradientului in functie de exemple aleatorii ale setului de date.
 
\end_layout

\begin_layout Standard
Batch Gradient Descent se diferentiaza de SGD prin faptul ca eroarea este
 calculata pentru fiecare exemplu din setul de date, dar ponderile sunt
 actualizate abia dupa ce toate datele au fost iterate.
 
\end_layout

\begin_layout Standard
O alta variatiune a algoritmului de Gradient Descent este Mini-batch Gradient
 Descent.
 Deoarece calcularea gradientilor pentru fiecare exemplu poate fi costisitor
 din punct de vedere computational, dar in acelasi timp memorarea gradientilor
 pentru intreg setul de date poate fi costisitor din punct de vedere al
 resurselor de stocare, Mini-batch Gradient Descent imparte setul de date
 in mai multe loturi, calculeaza eroarea modelului pentru fiecare lot si
 actualizeaza ponderile aferente.
 Aplicarea pasului de Gradiend Descent este decris in capitolul 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Gradient-Descent"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Momentum
\end_layout

\begin_layout Standard
Momentum
\begin_inset CommandInset citation
LatexCommand citep
key "Momentum"
literal "false"

\end_inset

 reprezinta o imbunatatire a algoritmului de Gradient Descent, ce ia in
 considerare gradientii precedenti si decide nivelul de contributie al acestora
 cat si al gradientului actual, folosind un parametru 
\begin_inset Formula $\beta$
\end_inset

 - factorul de descompunere exponențiala.
 Cu ajutorul acestui parametru se poate spune ca o medie locala exponential
 ponderata a gradientilor este calculata pentru actualizarea ponderilor
 modelului.
 Algoritmul este prezentat in 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Momentum"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout LyX-Code

\series bold
pentru
\series default
 iteratia 
\series bold

\begin_inset Formula $k$
\end_inset


\series default
:
\end_layout

\begin_deeper
\begin_layout LyX-Code
calculeaza gradientii 
\begin_inset Formula $\nabla E_{k}(\omega)$
\end_inset

 si 
\begin_inset Formula $\nabla E_{k}(b)$
\end_inset

 al lotului 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_layout LyX-Code
calculeaza mediile locale 
\begin_inset Formula $\varDelta\omega$
\end_inset

 si 
\begin_inset Formula $\varDelta b$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\varDelta\omega\leftarrow\beta\varDelta\omega+(1-\beta)\nabla E_{k}(\omega)$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\varDelta b\leftarrow\beta\varDelta b+(1-\beta)\nabla E_{k}(b)$
\end_inset


\end_layout

\end_deeper
\begin_layout LyX-Code
actualizeaza ponderile 
\begin_inset Formula $\omega^{*}$
\end_inset

 si 
\begin_inset Formula $b^{*}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\omega^{*}\leftarrow\omega-\alpha\varDelta\omega$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $b^{*}\leftarrow b-\alpha\varDelta b$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout LyX-Code

\series bold
sfarsit pentru
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Momentum"

\end_inset

Actualizarea ponderilor folosind Gradient Descent cu Momentum
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
RMSProp
\end_layout

\begin_layout Standard
Acronim pentru Root Mean Square Propagation, RMSProp
\begin_inset CommandInset citation
LatexCommand citep
key "RMSProp"
literal "false"

\end_inset

 imparte rata de invatare cu media locala a magnitudinilor gradientilor
 precedenti pentru ponderea in cauza.
 Se foloseste termenul 
\begin_inset Formula $\gamma$
\end_inset

 ca factor de uitare si 
\begin_inset Formula $\epsilon$
\end_inset

 drept scalar pentru prevenirea impartirii cu 0, operatiile efectuandu-se
 element-wise.
 Algoritmul este prezentat in 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:RMSProp"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout LyX-Code

\series bold
pentru
\series default
 iteratia 
\begin_inset Formula $k$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
calculeaza gradientii 
\begin_inset Formula $\nabla E_{k}(\omega)$
\end_inset

 si 
\begin_inset Formula $\nabla E_{k}(b)$
\end_inset

 al lotului 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_layout LyX-Code
calculeaza mediiile locale ale magnitudinii gradientilor 
\begin_inset Formula $\Theta\omega$
\end_inset

 si 
\begin_inset Formula $\Theta b$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\Theta\omega\leftarrow\gamma\Theta\omega+(1-\gamma)(\nabla E_{k}(\omega))^{2}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\Theta b\leftarrow\gamma\Theta b+(1-\gamma)(\nabla E_{k}(b))^{2}$
\end_inset


\end_layout

\end_deeper
\begin_layout LyX-Code
actualizeaza ponderile 
\begin_inset Formula $\omega^{*}$
\end_inset

 si 
\begin_inset Formula $b^{*}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\omega^{*}\leftarrow\omega-\alpha\frac{\nabla E_{k}(\omega)}{\sqrt{\Theta\omega}+\epsilon}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $b^{*}\leftarrow b-\alpha\frac{\nabla E_{k}(b)}{\sqrt{\Theta b}+\epsilon}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout LyX-Code

\series bold
sfarsit pentru
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:RMSProp"

\end_inset

Actualizarea ponderilor cu optimizatorul RMSProp
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Adam
\end_layout

\begin_layout Standard
Adaptive Moment Estimation, denumit si Adam, reprezinta aplicarea algoritmilor
 Momentum si RMSProp pe Gradient Descent.
 De asemenea, Adam efectueaza corectie de bias, ce are rolul de a atenua
 erorile in pasii incipienti antrenarii.
 Se pastreaza sintaxa prezentata mai sus.
 Algoritmul este prezentat in 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Adam"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout LyX-Code

\series bold
pentru
\series default
 iteratia 
\series bold

\begin_inset Formula $k$
\end_inset


\series default
:
\end_layout

\begin_deeper
\begin_layout LyX-Code
calculeaza gradientii 
\begin_inset Formula $\nabla E_{k}(\omega)$
\end_inset

 si 
\begin_inset Formula $\nabla E_{k}(b)$
\end_inset

 al lotului 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_layout LyX-Code
calculeaza 
\begin_inset Formula $\varDelta\omega$
\end_inset

,
\begin_inset Formula $\varDelta b$
\end_inset

,
\begin_inset Formula $\Theta\omega$
\end_inset

 si 
\begin_inset Formula $\Theta b$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\varDelta\omega\leftarrow\beta\varDelta\omega+(1-\beta)\nabla E_{k}(\omega)$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\varDelta b\leftarrow\beta\varDelta b+(1-\beta)\nabla E_{k}(b)$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\Theta\omega\leftarrow\gamma\Theta\omega+(1-\gamma)(\nabla E_{k}(\omega))^{2}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\Theta b\leftarrow\gamma\Theta b+(1-\gamma)(\nabla E_{k}(b))^{2}$
\end_inset


\end_layout

\end_deeper
\begin_layout LyX-Code
efectueaza corectie de bias:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\varDelta\omega^{corect}\leftarrow\frac{\varDelta\omega}{1-\beta^{k}}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\varDelta b^{corect}\leftarrow\frac{\varDelta b}{1-\beta^{k}}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\Theta\omega^{corect}\leftarrow\frac{\Theta\omega}{1-\gamma^{k}}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $\Theta b^{corect}\leftarrow\frac{\Theta b}{1-\gamma^{k}}$
\end_inset


\end_layout

\end_deeper
\begin_layout LyX-Code
actualizeaza ponderile 
\begin_inset Formula $\omega^{*}$
\end_inset

 si 
\begin_inset Formula $b^{*}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
\begin_inset Formula $\omega^{*}\leftarrow\omega-\alpha\frac{\varDelta\omega^{corect}}{\sqrt{\Theta\omega^{corect}}+\epsilon}$
\end_inset


\end_layout

\begin_layout LyX-Code
\begin_inset Formula $b^{*}\leftarrow b-\alpha\frac{\varDelta b^{corect}}{\sqrt{\Theta b^{corect}}+\epsilon}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout LyX-Code

\series bold
sfarsit pentru
\end_layout

\begin_layout LyX-Code

\series bold
returneaza
\series default
 parametrii 
\begin_inset Formula $\omega^{*},b^{*}$
\end_inset


\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Adam"

\end_inset

Actualizarea ponderilor cu optimizatorul Adam 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Rectified-Adam
\end_layout

\begin_layout Standard
O versiune imbunatatita fata de optimizatorul Adam, RAdam
\begin_inset CommandInset citation
LatexCommand citep
key "RAdam"
literal "false"

\end_inset

 aduce un grad de adaptabilitate ratei de invatare 
\begin_inset Formula $\alpha$
\end_inset

.
 In retelele neuronale, exista riscul ca optimizatorul sa convearga catre
 minime locale, demers ce poate aparea inca de la inceputul antrenarii si
 rezulta intr-o varianta crescuta.
 De aceea, autorii au descris o perioada de 
\begin_inset Quotes pld
\end_inset

incalzire
\begin_inset Quotes prd
\end_inset

 la inceputul antrenarii pentru rate de invatare, in care aceasta are valori
 scazute.
 
\end_layout

\begin_layout Standard
Calculand valoarea maxima 
\begin_inset Formula $\rho_{\infty}$
\end_inset

 si valoarea la o iteratie 
\begin_inset Formula $\rho_{k}$
\end_inset

 a mediei locale exponential ponderata si corectata, cu formulele 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Ro-infinit"
plural "false"
caps "false"
noprefix "false"

\end_inset

 si 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ro-iteratia-k"
plural "false"
caps "false"
noprefix "false"

\end_inset

, se defineste termenul de rectificare 
\begin_inset Formula $r_{k}$
\end_inset

 pentru iteratia 
\begin_inset Formula $k$
\end_inset

 cu formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:termen rectificare"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\rho_{\infty}=\frac{2}{1-\beta}-1\label{eq:Ro-infinit}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\rho_{k}=\rho_{\infty}-\frac{2k\beta^{k}}{1-\beta^{k}}\label{eq:ro-iteratia-k}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r_{k}=\sqrt{\frac{(\rho_{k}-4)(\rho_{k}-2)\rho_{\infty}}{(\rho_{\infty}-4)(\rho_{\infty}-2)\rho_{k}}}\label{eq:termen rectificare}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Termenul de rectificare se aplica pasilor de actualizare al ponderilor pana
 cand 
\begin_inset Formula $\rho_{k}\leq4$
\end_inset

 se adeverste.
 Valoarea 
\begin_inset Formula $4$
\end_inset

 a fost aleasa empiric.
 Este de mentionat cazul in care daca 
\begin_inset Formula $\rho_{\infty}\leq4$
\end_inset

 si 
\begin_inset Formula $\beta\leq0.6$
\end_inset

, RAdam degenereaza in algoritmul SGD cu Momentum.
\end_layout

\begin_layout Subsection
Lookahead
\end_layout

\begin_layout Standard
Creat relativ recent, Lookahead
\begin_inset CommandInset citation
LatexCommand citep
key "Lookahead"
literal "false"

\end_inset

 se diferentiaza de celelalte optimizatoare prezentate.
 In primul rand, acesta dispune de doua tipuri de ponderi: ponderi incete
 
\begin_inset Formula $\phi$
\end_inset

 si ponderi rapide 
\begin_inset Formula $\theta$
\end_inset

.
 Avand un punct de pornire 
\begin_inset Formula $\phi_{t}$
\end_inset

 memorat, algoritmul efectueaza 
\begin_inset Formula $k$
\end_inset

 pasi cu o variatie a SGD.
 Acesti pasi se considera ca se aplica pe ponderi rapide.
 Dupa cei 
\begin_inset Formula $k$
\end_inset

 pasi, functia de cost ajunge intr-un punct 
\begin_inset Formula $\theta_{k}$
\end_inset

.
 Avand 
\begin_inset Formula $\phi_{t}$
\end_inset

 , se efectueaza un pas in directia 
\begin_inset Formula $\theta_{k}$
\end_inset

, cu o pondere aleasa arbitrar 
\begin_inset Formula $\alpha$
\end_inset

.
 In alte cuvinte, algoritmul 
\begin_inset Quotes pld
\end_inset

se uita in fata
\begin_inset Quotes prd
\end_inset

, aplicand SGD, dupa care efectueaza un pas partial in directia rezultata.
 Algoritmul este prezentat in 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Lookahead"
plural "false"
caps "false"
noprefix "false"

\end_inset

, iar reprezentarea vizuala in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Lookahead reprezentare"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout LyX-Code
initializeaza parametrii 
\begin_inset Formula $\phi_{0}$
\end_inset

, 
\begin_inset Formula $k$
\end_inset

 si 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout LyX-Code
functia de cost 
\begin_inset Formula $L$
\end_inset

, optimizatorul 
\begin_inset Formula $A$
\end_inset


\end_layout

\begin_layout LyX-Code

\series bold
pentru
\series default
 pasii 
\begin_inset Formula $t=1,2,\ldots$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
sincronizeaza parametrii 
\begin_inset Formula $\theta_{t,0}\leftarrow\phi_{t-1}$
\end_inset


\end_layout

\begin_layout LyX-Code

\series bold
pentru
\series default
 pasii 
\begin_inset Formula $i=1,2,\ldots$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout LyX-Code
alege lotul 
\begin_inset Formula $d$
\end_inset


\end_layout

\begin_layout LyX-Code
actalizeaza ponderile rapide 
\begin_inset Formula $\theta_{t,i}\leftarrow\theta_{t,i-1}+A(L,\theta_{t,i-1},d)$
\end_inset


\end_layout

\end_deeper
\begin_layout LyX-Code

\series bold
sfarsit pentru
\end_layout

\begin_layout LyX-Code
actualizeaza ponderile incete 
\begin_inset Formula $\phi_{t}\leftarrow\phi_{t-1}+\alpha(\theta_{t,k}-\phi_{t-1})$
\end_inset


\end_layout

\end_deeper
\begin_layout LyX-Code

\series bold
sfarsit pentru
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Lookahead"

\end_inset

Actualizarea ponderilor cu optimizatorul Lookahead
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Lookahead.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Lookahead reprezentare"

\end_inset

Efectuarea a 2 pasi cu optimizatorul Lookahead
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ranger
\end_layout

\begin_layout Standard
Ranger
\begin_inset CommandInset citation
LatexCommand citep
key "Ranger"
literal "false"

\end_inset

 este o variatiune a optimizatorului Lookahead ce foloseste Rectified-Adam
 in loc de SGD.
 Aceasta combinatie este considerata complementara, intrucat RAdam prezinta
 imbunatari in starile incipiente ale procesului de antrenare, introducand
 o 
\begin_inset Quotes pld
\end_inset

incalzire
\begin_inset Quotes prd
\end_inset

 a gradientilor, iar Lookahead aduce in acest ansamblu stabilitatea antrenarii
 in stadiu avansat.
\end_layout

\begin_layout Section
Problemele Retelelor Neuronale si Solutii
\end_layout

\begin_layout Standard
Avand un numar substantial de parametrii si variabile ce trebuiesc ajustate,
 este aproape imposibil de a prevedea tipurile de probleme ce apar in timpul
 antrenarii unei retele neuronale.
 
\end_layout

\begin_layout Standard
De la probleme nou descoperite, unde comunitatea academica realizeaza studii
 sustinute de modele euristice de cercetare, la cele clasic intalnite in
 experimentele domeniului, unde este urmat un model cantitativ de cercetare,
 cert este ca inca exista loc pentru imbunatatiri.
\end_layout

\begin_layout Subsection
Varianță si Bias
\end_layout

\begin_layout Standard
In antrenarea unui model, varianta reprezinta cat de mult poate varia o
 predictie pentru o instanta de intrare nemaivazuta.
 Daca avem o varianta mare, inseamna ca modelul a invatat prea bine setul
 de date din timpul antrenarii si nu poate generaliza pentru date noi.
 Aceasta consecinta se numeste overfitting, si se manifesta printr-o eroare
 mica in datele de antrenare dar cu o eroare mare in datele de test.
\end_layout

\begin_layout Standard
Bias-ul pe de alta parte reprezinta gradul de simplicitate pe care il aplica
 modelul in predictiile sale.
 In cazul unui bias mare, modelul realizeaza multe presupuneri despre rezultatul
 dorit, ignorand caracteristicile prezente in setul de date, astfel simplificand
 prea mult solutia si nereusind sa o concretizeze.
 Aceasta consecinta se numeste underfitting, si se manifesta printr-o eroare
 mare atat pentru datele de antrenare, cat si cele de test.
\end_layout

\begin_layout Standard
Se concluzioneaza ca solutia optima prezinta atat o varianta cat si un bias
 scazut.
 Deoarece este dificila identificarea cauzei underfitting-ului, solutiile
 adoptate cauta sa evite complet cazul acesta si propun modele ce sunt predispus
e overfitting-ului.
 Astfel, scopul devine imbunatatirea abilitatii de generalizare a modelului.
\end_layout

\begin_layout Standard
Problema de overfitting apare in mod normal tarziu in procesul de antrenare.
 Pentru a fi detectata si evitata aceasta problema, este o practica buna
 sa se pastreze un istoric al erorii din timpul antrenarii cat si in timpul
 testarii.
 O solutie simpla se numeste early-stopping, ce detecteaza daca modelul
 incepe sa prezinte overfitting si opreste intreg procesul de antrenare.
 Deoarece minimele locale existente pot da impresia unui minim global, aceasta
 solutie nu este optima si trebuie ajustata pentru fiecare problema.
\end_layout

\begin_layout Subsection
Regularizare
\end_layout

\begin_layout Standard
Exista doua solutii pentru combaterea problemei de overfitting: imbogatirea
 setului de date sau aplicarea metodelor de regularizare.
 Astfel, regularizarea poate fi definita drept o tehnica de a imbunatati
 abilitatea unui model de a generaliza.
\end_layout

\begin_layout Subsubsection
Dropout
\end_layout

\begin_layout Standard
Una dintre cele mai folosite metode de regularizare este Dropout
\begin_inset CommandInset citation
LatexCommand citep
key "Dropout"
literal "false"

\end_inset

, reprezentata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dropout"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Aceasta metoda introduce pentru fiecare neuron din straturile ascunse probabili
tatea ca acesta sa fie anulat in timpul unui ciclu de antrenare.
 La prima vedere, acest procedeu pare contraintuitiv, intrucat o arhitectura
 mai complexa creste capacitatea de invatare a retelei.
 Cu toate acestea, exista cazuri in care la nivelul unui strat unii neuroni
 domina impactul pe care il au in calcularea predictilor, rezultand ca ceilalti
 neuroni sa nu poata invata caracteristicile mai subtile.
 Prin eliminarea neuronilor dominanti in unele cicluri de antrenare se permite
 dezvoltarea partilor ramase, astfel rezultand intr-un model ce generalizeaza
 mai bine.
 
\end_layout

\begin_layout Standard
Regularizarea Dropout se foloseste la straturile dense.
 In cazul imaginilor, informatia este continuta in regiuni, nu in puncte
 individuale.
 Aplicarea acestei metode va anula aleator puncte din imagine, dar nu va
 elimina caracteristici in totalitate.
 Batch Normalization prezinta acelasi un efect asemanator de regularizare
 cu Dropout, fapt pentru care este deseori folosit ca alternativa.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Dropout.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dropout"

\end_inset

Aplicare dropout pe doua straturi dense
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Regularizare L2
\end_layout

\begin_layout Standard
O alta metoda des folosita este regularizarea L2, ce penalizeaza ponderile
 mari in calculul valorii functiei de cost, adunand suma patratelor ponderilor
 inmultita cu termenul de regularizare 
\begin_inset Formula $\lambda$
\end_inset

.
 De exemplu, avand ponderile 
\begin_inset Formula $W=(w_{1},w_{2},\ldots,w_{m})$
\end_inset

 si formula de cost Cross-Entriopie Binara 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Cross-Entropy-Loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

, ne rezulta:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E=-\frac{1}{n}\sum_{k=1}^{n}((1-y_{k})ln(1-\hat{y}_{k})+y_{k}ln(\hat{y}_{k}))+\lambda(w_{1}^{2}+w_{2}^{2}+\ldots+w_{m}^{2})
\]

\end_inset


\end_layout

\begin_layout Subsubsection
DropBlock
\end_layout

\begin_layout Standard
Creat pentru Retelele Convolutionale, DropBlock
\begin_inset CommandInset citation
LatexCommand citep
key "DropBlock"
literal "false"

\end_inset

 anuleaza intregi regiuni dintr-o harta de caracteristici.
 Aplicarea acestei metoda are doi hiperparametrii: 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $block\text{\_}size$
\end_inset

 -
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 marimea laturii patratului regiunii anulate
\end_layout

\begin_layout Itemize
\begin_inset Formula $drop\text{\_}prob$
\end_inset

 - probabilitatea ca un punct sa fie ales pentru anulare
\end_layout

\begin_layout Standard
Avand o harta de caracteristici, se iau in considerare partile 
\begin_inset Quotes pld
\end_inset

activate
\begin_inset Quotes prd
\end_inset

 din aceasta, adica regiunile cu informatie relevanta.
 Pe punctele continute in aceasta regiune importanta se aplica o distributie
 Bernoulli cu probabilitatea 
\begin_inset Formula $\gamma$
\end_inset

.
 Deoarece doar punctele cu informatie vor fi alese, se foloseste termenul
 
\begin_inset Formula $\gamma$
\end_inset

 pentru a balansa lipsa alegerii punctelor neimportante.
 Acest termen are formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\gamma=\frac{drop\text{\_}prob}{block\text{\_}size^{2}}\frac{feat\text{\_}size^{2}}{(feat\text{\_}size-block\text{\_}size+1)^{2}}\label{eq:gamma_dropout}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Tehnica este exememplificata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DropBlock"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/DropBlock.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DropBlock"

\end_inset

Regularizare DropBlock
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Problema 
\begin_inset Quotes pld
\end_inset

dying ReLU
\begin_inset Quotes prd
\end_inset


\end_layout

\begin_layout Standard
In domeniul retelelor neuronale, saturatia este un termen ce descrie 
\begin_inset Quotes pld
\end_inset

tendinta orizontala
\begin_inset Quotes prd
\end_inset

 a unei functii.
 Cu cat o functie se apropie de valoarea la care converge, se poate spune
 ca functia isi pierde din puterea de antrenare pe care o poseda.
 Din aceste motive, lipsa limitei superioare a unei functii este o calitate
 dorita, intrucat problema saturatiei este inexistenta.
 
\end_layout

\begin_layout Standard
In cazul limitelor inferioare, se considera ca este contraintuitiv sa se
 aloce resurse pentru valori mari negative, ce sunt irelevante, intrucat
 reteaua trebuie sa detecteze doar caracteristicile ce determina rezultatul
 dorit, nu si cele incidental prezente.
 Existenta unei limite inferioare reduce din problema de overfitting, deci
 avand un efect regularizator.
 Este de mentionat faptul ca unele date pot fi procesate ca fiind irelevante
 devreme in procesul de antrenare, dar sa prezinte relevanta mai tarziu.
 Acest fapt justifica existenta tratarii valorilor negative in functiile
 moderne.
\end_layout

\begin_layout Standard
Se poate observa de ce ReLU este cea mai folosita functie de activare in
 interiorul retelelor neuronale: aceasta prezinta cel mai rapid timp de
 executie si nu are limita superioara.
 Dezavantajul major este faptul ca functia ReLU este complet saturata in
 domeniul negativ de valori.
 Acest detaliu duce la problema numita 
\begin_inset Quotes pld
\end_inset

dying ReLU
\begin_inset Quotes prd
\end_inset

.
 In alte cuvinte, valori ce sunt categorisite negative in stadiile incipiente
 ale procesului de invatare vor avea valoarea 0.
 Daca derivata pantei lui ReLU ajunge sa fie 0, intreg gradientul se va
 inmulti cu 0 si va nul, deci nu se va invata absolut nimic.
 In consecinta, odata ce un neuron cu activare ReLU 
\begin_inset Quotes pld
\end_inset

moare
\begin_inset Quotes prd
\end_inset

, acesta va ramane asa intotdeauna, intrucat derivata va fi 0 si neuronul
 nu va putea niciodata sa invete, lucru ce va cauza si limitarea puterii
 de invatare a neuronilor posteriori acestuia.
 
\end_layout

\begin_layout Standard
O rata de invatare prea mare poate duce la moartea prematura a neuronilor
 cu activare ReLU.
 Empiric, a fost dovedid ca pana la 40% din neuronii cu activare ReLU dintr-o
 retea pot suferi de aceasta problema, afectand puternic limitele de invatare
 ale retelei.
\end_layout

\begin_layout Standard
O solutie existenta este tratarea valorilor negative, fapt ce face ca derivata
 sa nu mai fie 0 si in consecinta un neuron cu activare ReLU ce a ajuns
 la valoarea 0 isi poate reveni in procesul de invatare.
 Aceasta solutie a fost intai adoptata de functii precum Leaky-ReLU si ELU,
 si a fost imbunatatia de functii precum Swish si Mish.
\end_layout

\begin_layout Chapter
Detalii de Implementare
\end_layout

\begin_layout Standard
Problema reconstructiei 3D a obiectelor din poze RGB prezinta aspecte ce
 nu pot fi rezolvate de metodele clasice existente
\begin_inset CommandInset citation
LatexCommand citep
key "Classic Reconstruction methods"
literal "false"

\end_inset

, unde pozitia pixelilor este triangulata.
 Unul dintre cele mai intalnite aspecte este reconstructia regiunilor ocluzate.
 Metodele de reconstructie folosesc de 3 tipuri de volume: ansamblu mesh,
 ansamblu de puncte si ansamblu de voxeli.
\end_layout

\begin_layout Standard
In reprezentarea voxelizata exista 3 variatii des folosite:
\end_layout

\begin_layout Enumerate
Grila ocupationala binara - un voxel este setat pe valoarea 1 daca face
 parte dintr-un obiect de interes, altfel este setat pe valoarea 0
\end_layout

\begin_layout Enumerate
Grila ocupationala probabilistica - in aceasta reprezentare fiecare voxel
 are o probabilitate de apartenenta la obiectul de interes
\end_layout

\begin_layout Enumerate
Signed Distance Function sau SDF - in fiecare voxel este codata distanta
 pana la cel mai apropiat punct de pe suprafata.
 O valoare negativa denota pozitia in interiorul obiectului iar una pozitiva
 pozitia pe suprafata.
\end_layout

\begin_layout Standard
Solutia implementata se numeste YOVO: You Only Voxelize Once 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Arhitectura-YOVO"
plural "false"
caps "false"
noprefix "false"

\end_inset

, denumire motivata de faptul ca aceasta trateaza doar reconstructia dintr-o
 singura imagine a obiectelor, oferind o reconstructie volumetrica voxelizata,
 atingand rezultate State-of-the-Art pe setul de date Data3D-R2N2.
 Pentru a trece peste aspectele unde metodele clasice prezinta dificultate,
 YOVO se foloseste de puterea si eficienta Retelelor Neuronale Convolutionale
 adanci, reusind sa invete caracteristici ascunse ale obiectului din imaginea
 de intrare pe baza informatiei existente despre obiecte din aceeasi clasa.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Architecture.png
	lyxscale 20
	scale 63

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Arhitectura YOVO
\begin_inset CommandInset label
LatexCommand label
name "fig:Arhitectura-YOVO"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Domeniul reconstructiei 3D a obiectelor prin invatare profunda este inca
 in stare incipienta, solutiile existente folosind in mod intensiv resurse
 in procesul de antrenare.
 Exista si metode de reconstructie cu rezolutie mare
\begin_inset CommandInset citation
LatexCommand citep
key "HighResolution 3D reconstruction"
literal "false"

\end_inset

, in care grila de voxeli are dimensiunea 
\begin_inset Formula $128^{3}$
\end_inset

.
 Aceasta performanta este realizata prin expansiunea retelei, dar prezentand
 costuri de memorie foarte mari, cresterea acestor costuri fiind cubuice.
 Din acest motiv, multe dintre solutiile de reconstructie a obiectelor este
 realizata la o scara mai mica, intr-un spatiu de 
\begin_inset Formula $32^{3}$
\end_inset

.
 YOVO produce volume in acest spatiu, realizand tranzitia de la o grila
 ocupationala probabilistica la una binara aplicand un threshold, intreg
 procesul avand o performanta ce se aproprie de domeniul reconstrutiei in
 timp real.
 
\end_layout

\begin_layout Standard
Inovatia solutiei prezenta in YOVO este extragerea multi-level eficientizata
 a caracteristicilor imaginii de intrare, extragand mai exact 3 seturi de
 caracteristici, fiecare captand aspecte mai mult sau mai putin generale
 ale obiectului prezent.
 De asemenea, unul dintre motivele pentru care aceasta solutie atinge noi
 performante State-of-the-Art este filtrarea informatiei in retea, ansamblul
 de functii de activare Mish si ELU fiind folosite complementar cu rolul
 modulelor in care sunt prezente.
 Aditional, aplicarea metodei de optimizare LookAhead cu Rectified Adam
 configurata cu hiperparametrii adaptabili, impreuna cu regularizare DropBlock,
 asigura o oarecare imunitate la overfitting si extinde limitele arhitecturii.
\end_layout

\begin_layout Section
Arhitectura
\end_layout

\begin_layout Standard
Arhitectura retelei YOVO este compusa din 4 componente.
 Pipeline-ul este urmatorul: se extrag multiple seturi de caracteristici,
 se reconstruieste fiecare set intr-un volum diferit, aceste volume sunt
 agregate, volumul rezultat este rafinat de un U-Net tridimensional.
 Structura detaliata este prezentata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Arhitectura-detaliata-YOVO"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Arhitectura Detaliata.png
	lyxscale 20
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Arhitectura-detaliata-YOVO"

\end_inset

Arhitectura detaliata YOVO
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Autoencoder
\end_layout

\begin_layout Standard
Rolul unui autoencoder este de a transforma un input intr-o reprezentare
 diferita, un ansamblu 3D.
 Aceasta structura are doua componente: 
\end_layout

\begin_layout Enumerate
un Encoder ce interpreteaza datele de intrare si le structureaza multiple
 seturi de harti de caracteristici
\end_layout

\begin_layout Enumerate
un Decoder ce reconstruieste rezultatul dorit pe baza informatiei abstracte
 prezente in multiplele seturi rezultate din Encoder
\end_layout

\begin_layout Subsubsection
Encoder
\end_layout

\begin_layout Standard
In solutia propusa, caracteristicile sunt intai extrase de un backbone format
 din primele 14 blocuri 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Backbone-MobileNetV2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 ale arhitecturii MobileNetV2
\begin_inset CommandInset citation
LatexCommand citep
key "Inverted Residuals and Linear Bottlenecks"
literal "false"

\end_inset

 .
 Prezenta blocurilor reziduale inversate asigura extragerea de caracteristici
 complexe intr-un numar mai mic de canale, proces eficientizat de convolutiile
 separabile.
 De asemenea, aplicarea straturilor de Batch Normalization au efect regularizato
r.
 Setul de caracteristici extras de backbone are dimensiunile 
\begin_inset Formula $(14,14,96)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Backbone.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Backbone-MobileNetV2"

\end_inset

Backbone MobileNetV2
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In continuare, sunt descrise trei nivele de abstractie la nivelul caracteristici
lor cuprinse in spatiul dimensional.
\end_layout

\begin_layout Enumerate
Primul nivel cuprinde informatia din spatiul dimensional mare (
\begin_inset Formula $512$
\end_inset

 canale) si o contine in 
\begin_inset Formula $256$
\end_inset

 canale
\end_layout

\begin_layout Enumerate
Al doilea nivel capteaza informatia dintr-un spatiu dimensional mediu (
\begin_inset Formula $320$
\end_inset

 de canale) si o codifica in 
\begin_inset Formula $256$
\end_inset

 canale
\end_layout

\begin_layout Enumerate
Al treilea nivel capteaza caracteristicile generale dintr-un spatiu dimensional
 mic (
\begin_inset Formula $128$
\end_inset

 de canale) si le cuprinde tot in 
\begin_inset Formula $256$
\end_inset

 de canale
\end_layout

\begin_layout Standard
Toate 3 nivelele produc seturi de caracteristici de dimensiunea 
\begin_inset Formula $(8,8,256)$
\end_inset

.
 Standardul de 
\begin_inset Formula $256$
\end_inset

 de canale este ales pentru a se reproduce volume in acelasi grila dimensionala.
 
\end_layout

\begin_layout Standard
Aceasta extragere multi-level este inspirata de versatilitatea arhitecturii
 FPN, dar adaptata pentru problema reconstruirii volumetrice.
 Fiecare nivel deriva din primul strat al nivelului precedent, fiind aplicate
 convolutii ce cresc numarul de canale dar reduc dimensiunile principale
 (lungime si latime) ale hartilor de caracteristici cu 
\begin_inset Formula $kernel\text{\_}size-1$
\end_inset

, din cauza lipsei aplicarii de padding.
 Intrucat informatia este extrasa in primele doua convolutii al fiecarui
 nivel, acestora le sunt aplicate regularizare DropBlock2D si activari ELU.
 Astfel, metoda DropBlock asigura un nivel de antrenare al caracteristicilor
 mai putin dominante, iar activarea ELU asigura o filtrare mai buna a caracteris
ticilor neimportante ale obiectului, acestea rezultand intotdeauna intr-o
 valoare negativa pentru a evita ambiguitatea creata atunci cand reteaua
 percepe apartenenta unui obiect la mai multe clase.
 In urma ultimei convolutii nu este aplicata regularizare DropBlock, dar
 activarea este setata drept Mish, intrucat in aceasta operatie informatia
 este codificata in 
\begin_inset Formula $256$
\end_inset

 de canale.
 Arhitectura detaliata a Autencoderului este prezentata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Autoencoder-YOVO"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/AutoEncoder.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Autoencoder-YOVO"

\end_inset

Autoencoderul introdus in YOVO
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Decoder
\end_layout

\begin_layout Standard
Hartile de caracteristici rezultate din Encoder, ce au structura 
\begin_inset Formula $(batch\text{\_}size,nr\text{\_}canale,lungime,latime)$
\end_inset

 sunt 
\begin_inset Quotes pld
\end_inset

aplatizate
\begin_inset Quotes prd
\end_inset

, adica reduse la un Tensor 1-Dimensional, apoi sunt restructurate in 
\begin_inset Formula $(batch\text{\_}size,nr\text{\_}canale,lungime,latime,inaltime)$
\end_inset

, adaugand o noua dimensiune.
 Astfel, Decoder-ul transforma informatia 2D intr-o interpretare 3D.
 Pentru a decoda caracteristicile, dimensionalitatea volumetrica 
\begin_inset Formula $(lungime,latime,inaltime)$
\end_inset

 trebuie sa creasca gradual.
 In consecinta, sunt folosite convolutii 3D transpuse, cu pas de 2.
 In total sunt folosite 5 straturi convolutionale.
 Primele 4 convolutii sunt evectuate cu kernel si pas neunitare, aplicand
 pe acestea Batch Normalization si activari Mish.
 Ultima convolutie este pointwise si are o activare Sigmoid, ce translateaza
 intreg ansamblul de caracteristici 3D intr-o grila ocupationala probabilistica.
 Numerele de canale aplicate pe fiecare convolutie sunt 
\begin_inset Formula $(512,128,32,8,1)$
\end_inset

.
 Dupa formarea volumului din ultimul strat, se concateneaza la acesta 8
 harti ce caracteristici precedente, ce vor avea rol contextual.
 
\end_layout

\begin_layout Standard
Decoder-ul produce 3 volume voxelizate de 
\begin_inset Formula $32^{3}$
\end_inset

 ale obiectului impreuna cu straturile contextuale ale acestora.
 Pentru Encoder si Decoder, este folosita functia de cost Cros-Entropie
 Binara dintre volumul real si cel recreat de intreg Autoencoder-ul.
\end_layout

\begin_layout Subsection
Agregator contextual
\end_layout

\begin_layout Standard
Un context este format dintr-un set de harti de caracteristici extrase din
 imaginea de intrare, care pe urma a fost interpretat de catre Decoder pana
 in penultimul strat din acesta.
 Intrucat extragerea de caracteristici este realizata multi-level de catre
 Encoder, cele 3 volume rezultate de Decoder sunt reconstruite avand contexte
 diferite.
 Fiecare context produce nivele de incredere diferite pentru regiunile volumetri
ce ale obiectului.
 De exemplu, un context poate produce un voxeli cu incredere mare pe suprafatele
 plane, altul poate produce voxeli ce reproduc cu incredere portiunile ascutite
 marginale.
 
\end_layout

\begin_layout Standard
Agregatorul contextual realizeaza contopirea volumelor in functie de contextul
 acestora.
 Pentru a realiza aceasta operatie, se vor crea ponderi pentru fiecare dintre
 cele 3 volume, mai exact pentru fiecare potential voxel continut in acestea.
 Aceste ponderi sunt produse de o retea convolutionala 3D, ce interpreteaza
 fiecare pereche de context cu volum.
 Ponderile sunt apoi normalizate intre toate 3 contexte cu o functie softmax
\end_layout

\begin_layout Standard
`
\begin_inset Formula 
\begin{equation}
s_{t}^{(i,j,k)}=\frac{e^{p_{t}^{(i,j,k)}}}{e^{p_{1}^{(i,j,k)}}+e^{p_{2}^{(i,j,k)}}+e^{p_{3}^{(i,j,k)}}}\label{eq:Softmax-Ponderi}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
unde 
\begin_inset Formula $s$
\end_inset

 este scorul unui singur voxel 
\begin_inset Formula $(i,j,k)$
\end_inset

 pentru contextul 
\begin_inset Formula $t$
\end_inset

 iar 
\begin_inset Formula $p$
\end_inset

 este ponderea voxelului.
 Intr-un final, se inmultesc cele 3 volume cu multimea de scoruri aferenta
 iar volumele rezultate sunt adunate element-wise, rezultand un singur volum
 intr-o grila ocupationala binara.
\end_layout

\begin_layout Standard
Structura acestui modul este prezentata in figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Agregator-contextual"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Agregator.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Agregatorul prezentat in YOVO
\begin_inset CommandInset label
LatexCommand label
name "fig:Agregator-contextual"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Rafinator
\end_layout

\begin_layout Standard
Prezenta sau lipsa unor ansamble mici de voxeli pot fi cauzate din cauza
 noise-ului .
 In cazul grilei dimensionale de 
\begin_inset Formula $32^{3}$
\end_inset

, asemenea ansamble pot avea un efect semniticativ in scaderea acuratetei
 modelului, aspect ce are mai putin impact in grile dimensionale mari.
 Din aceste motive, scopul acestui modul este de a imbunatati si retusa
 volumele produse de modulele anterioare.
 O analogie facuta de 
\begin_inset CommandInset citation
LatexCommand citep
key "Pix2Vox"
literal "false"

\end_inset

 face referinta la scopul blocurilor reziduale, in care captarea informatiei
 reziduale produce rezultate imbnatatite.
 
\end_layout

\begin_layout Standard
Acest modul consista intr-un 3D U-Net - un Autoencoder cu skip connections
 intre Encoder si Decoder.
 Encoderul este compus din 3 convolutii 3D cu efect de downsapling, rezultat
 din kernelul si pasul non-unitar.
 Trei straturi dense cu dropout prelucreaza informatia, dupa care aceasta
 trece prin alte 3 straturi de convolutii 3D transpuse.
 Intre straturile convolutionale ale Encoderului si Decoderului sunt efectuate
 skip-connections, pentru pastrarea informatiei.
 Pentru fiecare strat convolutional este aplicat Batch Normalization si
 activare mish, exceptie facand ultimul strat ce are activare Sigmoid.
\end_layout

\begin_layout Standard
Volumul rezultat este compus dintr-un ansamblu probabilistic de voxeli,
 pe care se aplica in paralel limite de 
\begin_inset Formula $0.2$
\end_inset

, 
\begin_inset Formula $0.3$
\end_inset

, 
\begin_inset Formula $0.4$
\end_inset

 si 
\begin_inset Formula $0.5$
\end_inset

, rezultand intr-un ansamblu binar.
 Pentru a se decide limita optima, se calculeaza acuratetea pentru fiecare
 in parte si se alege varianta cu cele mai bune rezultate.
\end_layout

\begin_layout Section
Set de date
\end_layout

\begin_layout Standard
Datasetul ShapeNet contine o multitudine de Proiectari 3D asistate de calculator
 (CAD).
 Este folosit un subset al acestui dataset, ce contine 43,783 de modele
 structurate in 13 clase ordonate alfabetic.
 Clasele sunt: aeroplane(avion), bench(banca), cabinet(dulap), car(masina),
 chair(scaun), display(monitor), lamp(lampa), speaker(boxa), rifle(arma),
 sofa(canapea), table(masa), telephone(telefon), watercraft(barca).
 Denumirile acestora sunt codificate in conformitate cu WordNet.
 
\end_layout

\begin_layout Section
Procesul de antrenare
\end_layout

\begin_layout Standard
Pentru a incepe procesul de antrenare, o configuratie trebuie stabilita.
 Retelei YOVO ii se pot seta anumiti parametrii direct din linia de comanda
 (e.g.
 denumirea experimentului, numarul de epoci, tipul rularii: testare/antrenare
 etc.), dar majoritatea dintre acestia sunt configurati intr-un fisier separat.
 Pentru simplicitatea accesarii valorilor din fisierul de configuratie,
 este folosita biblioteca easydict.
 De asemenea, YOVO salveaza progresul modelului la fiecare 10 epoci sau
 atunci cand performanta pe setul de validare atinge noi valori maxime.
\end_layout

\begin_layout Subsection
Preprocesarea datelor
\end_layout

\begin_layout Standard
Imaginile din datasetul ShapeNet au in medie dimensiunea de 137x137.
 Acestea sunt crop-uite la marimea 128x128 apoi redimensionate la 224x224.
 Pentru procesul de antrenare sunt aplicate metode de augmentare precum:
 flip aleator, culoare de fundal, permutare canale RGB, Normalizare, introducere
 de Noise.
 In schimb, pentru procesul de testare, doar normalizarea si schimbarea
 culorii de fundal sunt aplicate.
\end_layout

\begin_layout Subsection
Hiperparametrizare
\end_layout

\begin_layout Standard
Hiperparametrii existenti in YOVO fac referire atat la procesul de antrenare
 sau optimizare, cat si la celelalte metode folosite.
\end_layout

\begin_layout Standard
Sistemul este antrenat pentru 250 de epoci, cu marimea lotului 
\begin_inset Formula $batch\text{\_}size=32$
\end_inset

.
\end_layout

\begin_layout Standard
In algoritmul de optimizare Ranger, hiperparametrii au fost adaptati problemei
 actuale.
 
\end_layout

\begin_layout Itemize
RAdam: 
\begin_inset Formula $\beta=0.9$
\end_inset

, 
\begin_inset Formula $\gamma=0.999$
\end_inset

, 
\begin_inset Formula $\epsilon=10^{-8}$
\end_inset


\end_layout

\begin_layout Itemize
Lookahead: 
\begin_inset Formula $\alpha=0.5$
\end_inset

, 
\begin_inset Formula $k=6$
\end_inset


\end_layout

\begin_layout Standard
Pentru Encoder, Decoder si Rafinator avem rata de invatare 
\begin_inset Formula $lr=10^{-3}$
\end_inset

 iar pentru Agregator avem 
\begin_inset Formula $lr=10^{-4}$
\end_inset

.
 De asemenea, dupa 
\begin_inset Formula $150$
\end_inset

 de epoci ratele de invatare se inmultesc cu un factor de 
\begin_inset Formula $0.5$
\end_inset

.
\end_layout

\begin_layout Standard
Tehnicile de regularizare se aplica doar in timpul antrenarii.
 Pentru DropBlock avem 
\begin_inset Formula $drop\_prob=0.05$
\end_inset

 si 
\begin_inset Formula $block\_size=[1,2]$
\end_inset

, iar rata de dropout aplicata straturilor dense ale Rafinatorului este
 
\begin_inset Formula $0.1$
\end_inset

.
\end_layout

\begin_layout Subsection
Functia de cost si Metrica
\end_layout

\begin_layout Standard
Pentru a calcula eroarea in timpul antrenarii este folosita Cross-Entropia
 Binara Voxel Unitara 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Cost Cross-Entropic Binar Voxel-Unitar"
plural "false"
caps "false"
noprefix "false"

\end_inset

, iar metrica folosita pentru calculul acuratetei modelului este 3DIoU
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3D-IoU"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Validare si Testare
\end_layout

\begin_layout Standard
O practica des intalnita este impartirea setului de date atat pentru antrenare
 si testare, cat si pentru validare.
 Datele de validare sunt folosite pentru a evalua sistemul in timpul antrenarii,
 in scopul ajustarii hiperparametrilor.
 Datele de test au rolul de a evalua modelul final, pentru formarea unei
 analize comparative cu alte modele.
\end_layout

\begin_layout Standard
Din setul de date ShapeNet, 
\begin_inset Formula $30640$
\end_inset

 de imagini sunt folosite pentru antrenare, 
\begin_inset Formula $4371$
\end_inset

 pentru validare si 
\begin_inset Formula $8762$
\end_inset

 pentru testare.
\end_layout

\begin_layout Section
Mediul de antrenare
\end_layout

\begin_layout Subsection
Resurse Hardware
\end_layout

\begin_layout Standard
Antrenarea si rularea retelelor neuronale sunt procese ce necesita o cantitate
 mare de resurse atat computationale cat si de stocare.
 Reproducerea volumelor obiectelor din imagini 2D este o solutie cu o arhitectur
a complexa, ce prezinta un numar mare de ponderi antrenabile.
\end_layout

\begin_layout Standard
Din punct de vedere hardware, marimea spatiului de stocare, procesorul,
 placa video si memoria RAM sunt cele mai importante componente.
 Pentru problemele de Invatare Profunda nu este important daca setul de
 date este stocat pe un HDD sau pe un SSD.
 Procesorul este folosit in principal pentru preprocesarea datelor, numarul
 de nuclee si abilitatea de hyperthreading fiind esentiale in paralelizarea
 si accelerarea acestui proces.
 Marimea memoriei RAM va denota cat de multe date se vor putea aloca intr-o
 iteratie.
 Deoarece accesarea memoriei disk este seminificativ de inceata, se incearca
 incarcarea a cat mai multor date in memoria RAM, pentru a scadea numarul
 de accesari in memoria disk.
 Datele sunt transmise din memoria RAM in memoria placii video.
 Pentru a evita cazuri de limitare, este de preferat ca memoria RAM sa fie
 mare.
 Majoritatea operatilor realizate intr-o retea neuronala sunt inmultirile
 matriciale, operatie pentru care placile video sunt mai eficiente.
 Experimentele academice sunt dominate de placile NVIDIA, ce folosesc arhitectur
a software CUDA drept baza la antrenarea retelelor neuronale.
\end_layout

\begin_layout Standard
YOVO a fost antrenat timp de 60 de ore pe un sistem compus din: procesor
 Intel Core i7-6800K 3.40GHz, placa video NVIDIA RTX 2080Ti si memorie RAM
 24Gb DDR4.
 Referitor spatiului de stocare, este nevoie de 6.3Gb pentru setul de date,
 1.8Gb pentru modelul antrenat YOVO si 45Gb pentru salvarea modelelor intermediar
e in timpul antrenarii.
 In total, se recomanda cel putin 60Gb spatiu de stocare.
 
\end_layout

\begin_layout Subsection
Resurse Software
\end_layout

\begin_layout Standard
YOVO a fost scris in Python3, un limbaj de programare interpretat, de nivel
 inalt, cu accent pe simplicitate si intelegere usoara a codului.
 Versiunea softwareului de interactiune cu placa video NVIDIA este CUDA
 10.2.
 Frameworkul open-source folosit pentru manevrarea softwareului CUDA si
 pentru codarea modulelor specifice retelelor neuronale este PyTorch 1.5.
 Acesta aduce simplicitate procesul de experimentareh si prezinta atat o
 interfata in python cat si una in C++.
 Pentru a oferi versatilitate in experimentarea diferitelor solutii din
 invatarea profunda, este creat un mediu de antrenare virtual cu ajutorul
 softwareului Anaconda, in care se instaleaza urmatoarele biblioteci:
\end_layout

\begin_layout Enumerate
argsparse - parsarea parametrilor din linia de comanda
\end_layout

\begin_layout Enumerate
easydict - accesarea membrilor unui dictionar ca si atribute
\end_layout

\begin_layout Enumerate
matplotlib - salvarea si vizualizarea imaginilor si a videoclipurilor de
 exibitie
\end_layout

\begin_layout Enumerate
numpy - folosit preponderent pentru inmultirea matricilor multidimensionale
 mari
\end_layout

\begin_layout Enumerate
opencv-python - modificarea si procesarea de imagini
\end_layout

\begin_layout Enumerate
pprint - afisarea ordonata a configuratiei
\end_layout

\begin_layout Enumerate
scipy - citirea si manevrarea fisierelor tipice reprezentarii 3D
\end_layout

\begin_layout Enumerate
echoAI - contine implementarea functiei de activare Mish
\end_layout

\begin_layout Enumerate
pyglet - pachet necesar bibliotecii kaolin
\end_layout

\begin_layout Enumerate
kaolin - biblioteca pentru pentru reprezentare interactiva a obiectelor
 3D
\end_layout

\begin_layout Enumerate
dropblock - implementarea regularizarii dropblock 2D si 3D
\end_layout

\begin_layout Enumerate
tensorboardX - salvarea si vizualizarea statisticilor procesului de antrenare
\end_layout

\begin_layout Subsection
Vizualizare
\end_layout

\begin_layout Standard
YOVO poate fi configurat sa afiseze volumele reconstruite si cele reale,
 sa le salveze intr-un format specific aplicatiilor grafice 3D, sa le prezinte
 in 2 tipuri de medii interactive sau sa creeze un videoclip de exibitie
 in 360 de grade pentru acestea.
 O optiune pentru vizualizare interactiva este prin folosirea bibliotecii
 kaolin, ce prezinta volumele simplistic dar le poate converti si intr-un
 ansamblu mesh.
 Alternativa este vizualizarea interactiva folosind matplotlib, in care
 se poate observa grafic increderea pentru fiecare voxel din volumul reconstruit.
\end_layout

\begin_layout Chapter
Experimente si Rezultate
\end_layout

\begin_layout Standard
Arhitectura YOVO contine 466280683 de parametrii: 3921344 in Encoder, 71583064
 in Decoder, 8571 in Agregator si 39076704 in Rafinator.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Illustrations/Multi-Head-Volumes.png
	lyxscale 70
	scale 42

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Reproducere-multi-leve-voluemtric"

\end_inset

Reproducerea volumetrica pentru cele 3 nivele de abstractie
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
nr parametrii
\end_layout

\begin_layout Standard
specs
\end_layout

\begin_layout Standard
Accuracy measures the percentage correctness of the prediction i.e.
 correct−classestotal−classes
\end_layout

\begin_layout Standard
while
\end_layout

\begin_layout Standard
Loss actually tracks the inverse-confidence (for want of a better word)
 of the prediction.
 A high Loss score indicates that, even when the model is making good prediction
s, it is less
\end_layout

\begin_layout Standard
sure of the predictions it is making...and vice-versa.
\end_layout

\begin_layout Standard
So...
\end_layout

\begin_layout Standard
High Validation Accuracy + High Loss Score vs High Training Accuracy + Low
 Loss Score suggest that the model may be over-fitting on the training data.
 
\end_layout

\begin_layout Standard
TANH
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "3D-R2N2"
literal "false"

\end_inset

Christopher B.
 Choy and Danfei Xu and JunYoung Gwak and Kevin Chen and Silvio Savarese
 (2016).
 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object ReconstructionC
oRR, abs/1604.00449.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Pix2Vox"
literal "false"

\end_inset

Haozhe Xie and Hongxun Yao and Xiaoshuai Sun and Shangchen Zhou and Shengping
 Zhang and Xiaojun Tong (2019).
 Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view ImagesCoRR,
 abs/1901.11153.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SupervisedLearning"
literal "false"

\end_inset

Cunningham, Padraig & Cord, Matthieu & Delany, Sarah.
 (2008).
 Supervised Learning.
 10.1007/978-3-540-75171-7_2.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Backpropagation"
literal "false"

\end_inset

Goodfellow, Bengio & Courville (2016, 6.5 Back-Propagation and Other Differentiat
ion Algorithms, pp.
 200–220)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Tensorflow"
literal "false"

\end_inset

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
 G., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving,
 G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J.,
 Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,
 Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
 V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,
 X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),
 https://www.tensorflow.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Pytorch"
literal "false"

\end_inset

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
 Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in PyTorch.
 In: NeurIPS Autodiff Workshop (2017), https://pytorch.org/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ReteleConvDef"
literal "false"

\end_inset

Yamashita, R., Nishio, M., Do, R.K.G.
 et al.
 Convolutional neural networks: an overview and application in radiology.
 Insights Imaging 9, 611–629 (2018)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Separable_Conv"
literal "false"

\end_inset

Chollet, Francois.
 (2017).
 Xception: Deep Learning with Depthwise Separable Convolutions.
 1800-1807.
 10.1109/CVPR.2017.195.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Residual 1"
literal "false"

\end_inset

Kaiming He, Xiangyu Zhang, Shaoqing Ren, andJian Sun.
 Deep residual learning for image recog-nition.CoRR, abs/1512.03385, 2015.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Residual 2"
literal "false"

\end_inset

Saining Xie, Ross B.
 Girshick, Piotr Doll ́ar,Zhuowen Tu, and Kaiming He.Aggregatedresidual transform
ations for deep neural networks.CoRR, abs/1611.05431, 2016.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Inverted Residuals and Linear Bottlenecks"
literal "false"

\end_inset

Mark Sandler and Andrew G.
 Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen (2018).
 Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,
 Detection and SegmentationCoRR, abs/1801.04381.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "AutoEncoder"
literal "false"

\end_inset

D.
 E.
 Rumelhart, G.
 E.
 Hinton, and R.
 J.
 Williams.
 1986.
 Learning internal representations by error propagation.
 Parallel distributed processing: explorations in the microstructure of
 cognition, vol.
 1: foundations.
 MIT Press, Cambridge, MA, USA, 318–362.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "UNet"
literal "false"

\end_inset

Ronneberger, Olaf & Fischer, Philipp & Brox, Thomas.
 (2015).
 U-Net: Convolutional Networks for Biomedical Image Segmentation.
 arXiv:1505.04597v1
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "FPN"
literal "false"

\end_inset

Lin, Tsung-Yi et al.
 “Feature Pyramid Networks for Object Detection.” 2017 IEEE Conference on
 Computer Vision and Pattern Recognition (CVPR) (2017): 936-944.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "AutoML"
literal "false"

\end_inset

Marcel Wever, F.M., Hüllermeier, E.: ML-Plan for unlimited-length machine learning
 pipelines.
 In: Garnett, R., Vanschoren, F.H.J., Brazdil, P., Caruana, R., Giraud-Carrier,
 C., Guyon, I., Kégl, B.
 (eds.) ICML workshop on Automated Machine Learning (AutoML workshop 2018)
 (2018)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ReLU"
literal "false"

\end_inset

Glorot, X., Bordes, A.
 & Bengio, Y..
 (2011).
 Deep Sparse Rectifier Neural Networks.
 Proceedings of the Fourteenth International Conference on Artificial Intelligen
ce and Statistics, in PMLR 15:315-323 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Leaky-ReLU"
literal "false"

\end_inset

Maas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y.
 Rectifier nonlinearities improve neural network acoustic models.
 In Proc.
 ICML, volume 30, 2013.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ELU"
literal "false"

\end_inset

Clevert, Djork-Arné & Unterthiner, Thomas & Hochreiter, Sepp.
 (2016).
 Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Swish"
literal "false"

\end_inset

Prajit Ramachandran, Barret Zoph, & Quoc V.
 Le.
 (2017).
 Searching for Activation Functions.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Mish"
literal "false"

\end_inset

Diganta Misra.
 Mish: A self regularized nonmonotonic neural activation function.
 arXiv preprint arXiv:1908.08681, 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SGD"
literal "false"

\end_inset

Herbert E.
 Robbins (2007).
 A Stochastic Approximation MethodAnnals of Mathematical Statistics, 22,
 400-407.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Momentum"
literal "false"

\end_inset

David E.
 Rumelhart, Geoffrey E.
 Hinton, & Ronald J.
 Williams (1986).
 Learning representations by back-propagating errorsNature, 323, 533-536.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Deeplearning.ai"
literal "false"

\end_inset

Andrew Yan-Tak Ng.
 
\begin_inset Quotes pld
\end_inset

Deep Learning Specialization
\begin_inset Quotes prd
\end_inset

.
 https://www.deeplearning.ai/ (2017)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "RMSProp"
literal "false"

\end_inset

G.
 E.
 Hinton.
 "Lecture 6e RMSProp: Divide the gradient by a running average of its recent
 magnitude".
 https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Adam"
literal "false"

\end_inset

Kingma, Diederik & Ba, Jimmy.
 (2014).
 Adam: A Method for Stochastic Optimization.
 International Conference on Learning Representations.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "RAdam"
literal "false"

\end_inset

Liu, Liyuan & Jiang, Haoming & He, Pengcheng & Chen, Weizhu & Liu, Xiaodong
 & Gao, Jianfeng & Han, Jiawei.
 (2019).
 On the Variance of the Adaptive Learning Rate and Beyond.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Lookahead"
literal "false"

\end_inset

Michael R.
 Zhang and James Lucas and Geoffrey E.
 Hinton and Jimmy Ba (2019).
 Lookahead Optimizer: k steps forward, 1 step backCoRR, abs/1907.08610.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Ranger"
literal "false"

\end_inset

Less Wright (2019).
 New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam +
 LookAhead for the best of both.
 https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combina
tion-of-radam-lookahead-for-the-best-of-2dc83f79a48d
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Dropout"
literal "false"

\end_inset

G.
 E.
 Hinton, N.
 Srivastava, A.
 Krizhevsky, I.
 Sutskever, and R.
 R.
 Salakhutdinov, “Improving neural networks by preventing co-adaptation of
 feature detectors,” arXiv preprint arXiv:1207.0580, 2012.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "DropBlock"
literal "false"

\end_inset

Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le.
 DropBlock: A regularization method for convolutional networks.
 In Advances in Neural Information Processing Systems (NIPS), pages 10727–10737,
 2018.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Classic Reconstruction methods"
literal "false"

\end_inset

R.
 Hartley and A.
 Zisserman,Multiple view geometry in computervision.
 Cambridge university press, 2003.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "HighResolution 3D reconstruction"
literal "false"

\end_inset

J.
 Wu, Y.
 Wang, T.
 Xue, X.
 Sun, B.
 Freeman, and J.
 Tenenbaum,“MarrNet: 3D shape reconstruction via 2.5D sketches,” inNIPS,2017,
 pp.
 540–550.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "SotA trends 3D-Reconstruction"
literal "false"

\end_inset

X.
 Han, H.
 Laga and M.
 Bennamoun, "Image-based 3D Object Reconstruction: State-of-the-Art and
 Trends in the Deep Learning Era," in IEEE Transactions on Pattern Analysis
 and Machine Intelligence, doi: 10.1109/TPAMI.2019.2954885.
\end_layout

\end_body
\end_document
